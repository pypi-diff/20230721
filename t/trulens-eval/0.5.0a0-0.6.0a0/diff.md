# Comparing `tmp/trulens_eval-0.5.0a0-py3-none-any.whl.zip` & `tmp/trulens_eval-0.6.0a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,46 @@
-Zip file size: 247739 bytes, number of entries: 42
--rw-rw-r--  2.0 unx     5164 b- defN 23-Jul-12 14:37 trulens_eval/Example_TruBot.py
--rw-rw-r--  2.0 unx     3265 b- defN 23-Jul-12 14:37 trulens_eval/Leaderboard.py
--rw-rw-r--  2.0 unx     1371 b- defN 23-Jul-12 22:46 trulens_eval/__init__.py
--rw-rw-r--  2.0 unx    11727 b- defN 23-Jul-12 20:22 trulens_eval/app.py
+Zip file size: 254678 bytes, number of entries: 44
+-rw-rw-r--  2.0 unx     5199 b- defN 23-Jul-21 19:10 trulens_eval/Example_TruBot.py
+-rw-rw-r--  2.0 unx     3045 b- defN 23-Jul-21 19:10 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx     1424 b- defN 23-Jul-21 19:20 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx    12040 b- defN 23-Jul-21 19:10 trulens_eval/app.py
 -rw-rw-r--  2.0 unx     5344 b- defN 23-Jul-12 14:37 trulens_eval/benchmark.py
--rw-rw-r--  2.0 unx    21071 b- defN 23-Jul-12 14:37 trulens_eval/db.py
+-rw-rw-r--  2.0 unx    21224 b- defN 23-Jul-21 19:10 trulens_eval/db.py
 -rw-rw-r--  2.0 unx    14061 b- defN 23-Jul-12 14:37 trulens_eval/db_migration.py
--rw-rw-r--  2.0 unx    48952 b- defN 23-Jul-12 18:13 trulens_eval/feedback.py
--rw-rw-r--  2.0 unx     3443 b- defN 23-Jul-12 14:37 trulens_eval/feedback_prompts.py
+-rw-rw-r--  2.0 unx    56377 b- defN 23-Jul-21 19:10 trulens_eval/feedback.py
+-rw-rw-r--  2.0 unx     5420 b- defN 23-Jul-21 19:10 trulens_eval/feedback_prompts.py
 -rw-rw-r--  2.0 unx    20145 b- defN 23-Jul-12 18:13 trulens_eval/instruments.py
--rw-rw-r--  2.0 unx     3147 b- defN 23-Jul-12 14:37 trulens_eval/keys.py
--rw-rw-r--  2.0 unx    21353 b- defN 23-Jul-12 14:37 trulens_eval/provider_apis.py
--rw-rw-r--  2.0 unx    13750 b- defN 23-Jul-12 20:22 trulens_eval/schema.py
--rw-rw-r--  2.0 unx    16226 b- defN 23-Jul-12 14:37 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx    13286 b- defN 23-Jul-21 19:10 trulens_eval/keys.py
+-rw-rw-r--  2.0 unx    23180 b- defN 23-Jul-21 19:10 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    13929 b- defN 23-Jul-21 19:10 trulens_eval/schema.py
+-rw-rw-r--  2.0 unx    16238 b- defN 23-Jul-21 19:10 trulens_eval/tru.py
 -rw-rw-r--  2.0 unx      293 b- defN 23-Jul-12 14:37 trulens_eval/tru_app.py
 -rw-rw-r--  2.0 unx     3545 b- defN 23-Jul-12 18:13 trulens_eval/tru_basic_app.py
 -rw-rw-r--  2.0 unx     7819 b- defN 23-Jul-12 20:22 trulens_eval/tru_chain.py
 -rw-rw-r--  2.0 unx      288 b- defN 23-Jul-12 14:37 trulens_eval/tru_db.py
 -rw-rw-r--  2.0 unx      318 b- defN 23-Jul-12 14:37 trulens_eval/tru_feedback.py
--rw-rw-r--  2.0 unx     6370 b- defN 23-Jul-12 14:37 trulens_eval/tru_llama.py
--rw-rw-r--  2.0 unx    43449 b- defN 23-Jul-12 20:22 trulens_eval/util.py
--rw-rw-r--  2.0 unx    12673 b- defN 23-Jul-12 20:22 trulens_eval/pages/Evaluations.py
--rw-rw-r--  2.0 unx     1775 b- defN 23-Jul-12 14:37 trulens_eval/pages/Progress.py
+-rw-rw-r--  2.0 unx     6396 b- defN 23-Jul-21 19:10 trulens_eval/tru_llama.py
+-rw-rw-r--  2.0 unx    47832 b- defN 23-Jul-21 19:10 trulens_eval/util.py
+-rw-rw-r--  2.0 unx    12802 b- defN 23-Jul-21 19:10 trulens_eval/pages/Evaluations.py
+-rw-rw-r--  2.0 unx     1848 b- defN 23-Jul-21 19:10 trulens_eval/pages/Progress.py
 -rw-rw-r--  2.0 unx     3294 b- defN 23-Jul-12 20:22 trulens_eval/react_components/record_viewer/__init__.py
--rw-rw-r--  2.0 unx      411 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/index.html
--rw-rw-r--  2.0 unx   470039 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js
--rw-rw-r--  2.0 unx      779 b- defN 23-Jul-12 22:47 trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css
--rw-rw-r--  2.0 unx     5551 b- defN 23-Jul-12 14:37 trulens_eval/tests/test_tru_chain.py
+-rw-rw-r--  2.0 unx      411 b- defN 23-Jul-21 19:21 trulens_eval/react_components/record_viewer/dist/index.html
+-rw-rw-r--  2.0 unx   470039 b- defN 23-Jul-21 19:21 trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js
+-rw-rw-r--  2.0 unx      779 b- defN 23-Jul-21 19:21 trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-21 19:10 trulens_eval/utils/__init__.py
 -rw-rw-r--  2.0 unx       83 b- defN 23-Jul-12 20:22 trulens_eval/utils/command_line.py
 -rw-rw-r--  2.0 unx     5892 b- defN 23-Jul-12 22:23 trulens_eval/utils/langchain.py
--rw-rw-r--  2.0 unx     4842 b- defN 23-Jul-12 14:37 trulens_eval/utils/llama.py
+-rw-rw-r--  2.0 unx     4831 b- defN 23-Jul-21 19:10 trulens_eval/utils/llama.py
 -rw-rw-r--  2.0 unx     1001 b- defN 23-Jul-12 14:37 trulens_eval/utils/notebook_utils.py
+-rw-rw-r--  2.0 unx      151 b- defN 23-Jul-21 19:10 trulens_eval/utils/python.py
+-rw-rw-r--  2.0 unx      166 b- defN 23-Jul-21 19:10 trulens_eval/utils/text.py
 -rw-rw-r--  2.0 unx      927 b- defN 23-Jul-12 20:22 trulens_eval/utils/trulens.py
 -rw-rw-r--  2.0 unx     1212 b- defN 23-Jul-12 14:37 trulens_eval/ux/add_logo.py
--rw-rw-r--  2.0 unx     5492 b- defN 23-Jul-12 20:22 trulens_eval/ux/components.py
--rw-rw-r--  2.0 unx     1209 b- defN 23-Jul-12 14:37 trulens_eval/ux/styles.py
+-rw-rw-r--  2.0 unx     5847 b- defN 23-Jul-21 19:10 trulens_eval/ux/components.py
+-rw-rw-r--  2.0 unx     2306 b- defN 23-Jul-21 19:10 trulens_eval/ux/styles.py
 -rw-rw-r--  2.0 unx    29567 b- defN 23-Jul-12 14:37 trulens_eval/ux/trulens_logo.svg
--rw-rw-r--  2.0 unx    16347 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       70 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       13 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3689 b- defN 23-Jul-12 22:47 trulens_eval-0.5.0a0.dist-info/RECORD
-42 files, 816059 bytes uncompressed, 241827 bytes compressed:  70.4%
+-rw-rw-r--  2.0 unx    16346 b- defN 23-Jul-21 19:21 trulens_eval-0.6.0a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-21 19:21 trulens_eval-0.6.0a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       70 b- defN 23-Jul-21 19:21 trulens_eval-0.6.0a0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Jul-21 19:21 trulens_eval-0.6.0a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3847 b- defN 23-Jul-21 19:21 trulens_eval-0.6.0a0.dist-info/RECORD
+44 files, 838927 bytes uncompressed, 248518 bytes compressed:  70.4%
```

## zipnote {}

```diff
@@ -75,29 +75,35 @@
 
 Filename: trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js
 Comment: 
 
 Filename: trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css
 Comment: 
 
-Filename: trulens_eval/tests/test_tru_chain.py
+Filename: trulens_eval/utils/__init__.py
 Comment: 
 
 Filename: trulens_eval/utils/command_line.py
 Comment: 
 
 Filename: trulens_eval/utils/langchain.py
 Comment: 
 
 Filename: trulens_eval/utils/llama.py
 Comment: 
 
 Filename: trulens_eval/utils/notebook_utils.py
 Comment: 
 
+Filename: trulens_eval/utils/python.py
+Comment: 
+
+Filename: trulens_eval/utils/text.py
+Comment: 
+
 Filename: trulens_eval/utils/trulens.py
 Comment: 
 
 Filename: trulens_eval/ux/add_logo.py
 Comment: 
 
 Filename: trulens_eval/ux/components.py
@@ -105,23 +111,23 @@
 
 Filename: trulens_eval/ux/styles.py
 Comment: 
 
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.5.0a0.dist-info/METADATA
+Filename: trulens_eval-0.6.0a0.dist-info/METADATA
 Comment: 
 
-Filename: trulens_eval-0.5.0a0.dist-info/WHEEL
+Filename: trulens_eval-0.6.0a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.5.0a0.dist-info/entry_points.txt
+Filename: trulens_eval-0.6.0a0.dist-info/entry_points.txt
 Comment: 
 
-Filename: trulens_eval-0.5.0a0.dist-info/top_level.txt
+Filename: trulens_eval-0.6.0a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.5.0a0.dist-info/RECORD
+Filename: trulens_eval-0.6.0a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/Example_TruBot.py

```diff
@@ -14,29 +14,33 @@
 
 from trulens_eval import feedback
 from trulens_eval import Query
 from trulens_eval import tru
 from trulens_eval import tru_chain
 from trulens_eval.db import Record
 from trulens_eval.feedback import Feedback
-from trulens_eval.keys import *
-from trulens_eval.keys import PINECONE_API_KEY
-from trulens_eval.keys import PINECONE_ENV
+from trulens_eval.keys import check_keys
+
+check_keys(
+    "OPENAI_API_KEY",
+    "PINECONE_API_KEY",
+    "PINECONE_ENV"
+)
 
 # Set up GPT-3 model
 model_name = "gpt-3.5-turbo"
 
 app_id = "TruBot"
 # app_id = "TruBot_langprompt"
 # app_id = "TruBot_relevance"
 
 # Pinecone configuration.
 pinecone.init(
-    api_key=PINECONE_API_KEY,  # find at app.pinecone.io
-    environment=PINECONE_ENV  # next to api key in console
+    api_key=os.environ.get("PINECONE_API_KEY"),  # find at app.pinecone.io
+    environment=os.environ.get("PINECONE_ENV")  # next to api key in console
 )
 
 identity = lambda h: h
 
 hugs = feedback.Huggingface()
 openai = feedback.OpenAI()
```

## trulens_eval/Leaderboard.py

```diff
@@ -2,20 +2,20 @@
 
 from millify import millify
 import numpy as np
 import streamlit as st
 from streamlit_extras.switch_page_button import switch_page
 
 from trulens_eval.db_migration import MIGRATION_UNKNOWN_STR
+from trulens_eval.ux.styles import CATEGORY
 
 st.runtime.legacy_caching.clear_cache()
 
 from trulens_eval import db
 from trulens_eval import Tru
-from trulens_eval.feedback import default_pass_fail_color_threshold
 from trulens_eval.ux import styles
 
 st.set_page_config(page_title="Leaderboard", layout="wide")
 
 from trulens_eval.ux.add_logo import add_logo
 
 add_logo()
@@ -84,27 +84,21 @@
                 unsafe_allow_html=True,
             )
 
             if math.isnan(mean):
                 pass
 
             else:
-                if mean >= default_pass_fail_color_threshold:
-                    feedback_cols[i].metric(
-                        label=col_name,
-                        value=f'{round(mean, 2)}',
-                        delta='✅ High'
-                    )
-                else:
-                    feedback_cols[i].metric(
-                        label=col_name,
-                        value=f'{round(mean, 2)}',
-                        delta='⚠️ Low ',
-                        delta_color="inverse"
-                    )
+                cat = CATEGORY.of_score(mean)
+                feedback_cols[i].metric(
+                    label=col_name,
+                    value=f'{round(mean, 2)}',
+                    delta=f'{cat.icon} {cat.adjective}',
+                    delta_color="normal" if mean >= CATEGORY.PASS.threshold else "inverse"
+                )
 
         with col99:
             if st.button('Select App', key=f"app-selector-{app}"):
                 st.session_state.app = app
                 switch_page('Evaluations')
 
         st.markdown("""---""")
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## trulens_eval/__init__.py

```diff
@@ -26,18 +26,22 @@
 
     - `instruments.py`
 
     - `provider_apis.py` `feedback_prompts.py`
 
     - `schema.py`
 
-    - `util.py` `keys.py`
+    - `util.py` 
+    
+    - `keys.py`
+
+    - `utils/python.py` `utils/text.py`
 """
 
-__version__ = "0.5.0a"
+__version__ = "0.6.0a"
 
 from trulens_eval.feedback import Feedback
 from trulens_eval.feedback import Huggingface
 from trulens_eval.feedback import OpenAI
 from trulens_eval.feedback import Provider
 from trulens_eval.schema import FeedbackMode
 from trulens_eval.schema import Query
```

## trulens_eval/app.py

```diff
@@ -272,14 +272,21 @@
 
         else:
             if len(feedbacks) > 0:
                 raise ValueError(
                     "Feedback logging requires `tru` to be specified."
                 )
 
+        if self.feedback_mode == FeedbackMode.DEFERRED:
+            for f in feedbacks:
+                # Try to load each of the feedback implementations. Deferred
+                # mode will do this but we want to fail earlier at app
+                # constructor here.
+                f.implementation.load()
+
         self.instrument.instrument_object(
             obj=self.app, query=Select.Query().app
         )
 
     def json(self, *args, **kwargs):
         # Need custom jsonification here because it is likely the model
         # structure contains loops.
```

## trulens_eval/db.py

```diff
@@ -26,16 +26,16 @@
 from trulens_eval.schema import FeedbackResultStatus
 from trulens_eval.schema import Perf
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordID
 from trulens_eval.util import JSON
 from trulens_eval.util import json_str_of_obj
 from trulens_eval.util import SerialModel
-from trulens_eval.util import UNICODE_CHECK
-from trulens_eval.util import UNICODE_CLOCK
+from trulens_eval.utils.text import UNICODE_CHECK
+from trulens_eval.utils.text import UNICODE_CLOCK
 
 mj = MerkleJson()
 NoneType = type(None)
 
 pp = PrettyPrinter()
 
 logger = logging.getLogger(__name__)
@@ -160,14 +160,15 @@
     TABLE_RECORDS = "records"
     TABLE_FEEDBACKS = "feedbacks"
     TABLE_FEEDBACK_DEFS = "feedback_defs"
     TABLE_APPS = "apps"
 
     TYPE_TIMESTAMP = "FLOAT"
     TYPE_ENUM = "TEXT"
+    TYPE_JSON = "TEXT"
 
     TABLES = [TABLE_RECORDS, TABLE_FEEDBACKS, TABLE_FEEDBACK_DEFS, TABLE_APPS]
 
     def __init__(self, filename: Path):
         """
         Database locally hosted using SQLite.
 
@@ -209,15 +210,15 @@
 
         self._close(conn)
 
     def get_meta(self):
         conn, c = self._connect()
 
         try:
-            c.execute(f'''SELECT key, value from {self.TABLE_META}''')
+            c.execute(f'''SELECT key, value FROM {self.TABLE_META}''')
             rows = c.fetchall()
             ret = {}
 
             for row in rows:
                 ret[row[0]] = row[1]
 
             if 'trulens_version' in ret:
@@ -241,16 +242,18 @@
         # also encodes inside it all other columns.
 
         meta = self.get_meta()
 
         if meta.trulens_version is None:
             db_version = __version__
             c.execute(
-                f"""SELECT name FROM sqlite_master  
-                WHERE type='table';"""
+                f"""
+                SELECT name FROM sqlite_master  
+                WHERE type='table';
+                """
             )
             rows = c.fetchall()
 
             if len(rows) > 1:
                 # _create_db_meta_table is called before any DB manipulations,
                 # so if existing tables are present but it's an empty metatable, it means this is trulens-eval first release.
                 db_version = "0.1.2"
@@ -265,45 +268,45 @@
         self._create_db_meta_table(c)
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_RECORDS} (
                 record_id TEXT NOT NULL PRIMARY KEY,
                 app_id TEXT NOT NULL,
                 input TEXT,
                 output TEXT,
-                record_json TEXT NOT NULL,
+                record_json {self.TYPE_JSON} NOT NULL,
                 tags TEXT NOT NULL,
                 ts {self.TYPE_TIMESTAMP} NOT NULL,
-                cost_json TEXT NOT NULL,
-                perf_json TEXT NOT NULL
+                cost_json {self.TYPE_JSON} NOT NULL,
+                perf_json {self.TYPE_JSON} NOT NULL
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACKS} (
                 feedback_result_id TEXT NOT NULL PRIMARY KEY,
                 record_id TEXT NOT NULL,
                 feedback_definition_id TEXT,
                 last_ts {self.TYPE_TIMESTAMP} NOT NULL,
                 status {self.TYPE_ENUM} NOT NULL,
                 error TEXT,
-                calls_json TEXT NOT NULL,
+                calls_json {self.TYPE_JSON} NOT NULL,
                 result FLOAT,
                 name TEXT NOT NULL,
-                cost_json TEXT NOT NULL
+                cost_json {self.TYPE_JSON} NOT NULL
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_FEEDBACK_DEFS} (
                 feedback_definition_id TEXT NOT NULL PRIMARY KEY,
-                feedback_json TEXT NOT NULL
+                feedback_json {self.TYPE_JSON} NOT NULL
             )'''
         )
         c.execute(
             f'''CREATE TABLE IF NOT EXISTS {self.TABLE_APPS} (
                 app_id TEXT NOT NULL PRIMARY KEY,
-                app_json TEXT NOT NULL
+                app_json {self.TYPE_JSON} NOT NULL
             )'''
         )
         self._close(conn)
 
     def _connect(self) -> Tuple[sqlite3.Connection, sqlite3.Cursor]:
         conn = sqlite3.connect(self.filename)
         c = conn.cursor()
```

## trulens_eval/feedback.py

```diff
@@ -3,17 +3,17 @@
 
 The `Feedback` class contains the starting point for feedback function
 specification and evaluation. A typical use-case looks like this:
 
 ```python 
 from trulens_eval import feedback, Select, Feedback
 
-openai = feedback.OpenAI()
+hugs = feedback.Huggingface()
 
-f_lang_match = Feedback(openai.language_match)
+f_lang_match = Feedback(hugs.language_match)
     .on_input_output()
 ```
 
 The components of this specifications are:
 
 - **Provider classes** -- `feedback.OpenAI` contains feedback function
   implementations like `qs_relevance`. Other classes subtyping
@@ -114,41 +114,45 @@
 
 ## Specifying Implementation Function and Aggregate
 
 The function or method provided to the `Feedback` constructor is the
 implementation of the feedback function which does the actual work of producing
 a float indicating some quantity of interest. 
 
-**Note regarding FeedbackMode.DEFERRED** -- Any callable can be provided here
-but there are additional requirements if your app uses the "deferred" feedback
-evaluation mode (when `feedback_mode=FeedbackMode.DEFERRED` are specified to app
-constructor). In those cases the callables must be methods that are globally
-importable (see the next section for details). The function/method performing
-the aggregation has the same requirements.
+**Note regarding FeedbackMode.DEFERRED** -- Any function or method (not static
+or class methods presently supported) can be provided here but there are
+additional requirements if your app uses the "deferred" feedback evaluation mode
+(when `feedback_mode=FeedbackMode.DEFERRED` are specified to app constructor).
+In those cases the callables must be functions or methods that are importable
+(see the next section for details). The function/method performing the
+aggregation has the same requirements.
 
-### Global import requirement (DEFERRED feedback mode only)
+### Import requirement (DEFERRED feedback mode only)
 
 If using deferred evaluation, the feedback function implementations and
-aggregation implementations must be methods from a class that is globally
-importable. That is, the callables must be accessible were you to evaluate this
-code:
+aggregation implementations must be functions or methods from a Provider
+subclass that is importable. That is, the callables must be accessible were you
+to evaluate this code:
 
 ```python
-import somepackage.[...].someclass 
+from somepackage.[...] import someproviderclass
+from somepackage.[...] import somefunction
+
 # [...] means optionally further package specifications
 
-provider = someclass(...) # constructor arguments can be included
-feedback_implementation = provider.somemethod
+provider = someproviderclass(...) # constructor arguments can be included
+feedback_implementation1 = provider.somemethod
+feedback_implementation2 = somefunction
 ```
 
 For provided feedback functions, `somepackage` is `trulens_eval.feedback` and
-`someclass` is `OpenAI` or one of the other `Provider` subclasses. Custom
-feedback functions likewise need to belong to a package that can be imported.
-Critically, classes defined locally in a notebook will not be importable this
-way.
+`someproviderclass` is `OpenAI` or one of the other `Provider` subclasses.
+Custom feedback functions likewise need to be importable functions or methods of
+a provider subclass that can be imported. Critically, functions or classes
+defined locally in a notebook will not be importable this way.
 
 ## Specifying Arguments
 
 The mapping between app/records to feedback implementation arguments is
 specified by the `on...` methods of the `Feedback` objects. The general form is:
 
 ```python
@@ -160,21 +164,21 @@
 argument names. The types of `selector1` is `JSONPath` which we elaborate on in
 the "Selector Details".
 
 If argument names are ommitted, they are taken from the feedback function
 implementation signature in order. That is, 
 
 ```python
-...on(argname1=selector1, argname2=selector2)
+Feedback(...).on(argname1=selector1, argname2=selector2)
 ```
 
 and
 
 ```python
-...on(selector1, selector2)
+Feedback(...).on(selector1, selector2)
 ```
 
 are equivalent assuming the feedback implementation has two arguments,
 `argname1` and `argname2`, in that order.
 
 ### Running Feedback
 
@@ -395,15 +399,16 @@
 from inspect import Signature
 from inspect import signature
 import itertools
 import logging
 from multiprocessing.pool import AsyncResult
 import re
 import traceback
-from typing import Any, Callable, Dict, Iterable, Optional, Type, Union
+from typing import (Any, Callable, Dict, Iterable, List, Optional, Tuple, Type,
+                    Union)
 
 import numpy as np
 import openai
 import pydantic
 
 from trulens_eval import feedback_prompts
 from trulens_eval.keys import *
@@ -420,83 +425,108 @@
 from trulens_eval.schema import Record
 from trulens_eval.schema import Select
 from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import JSON
 from trulens_eval.util import jsonify
 from trulens_eval.util import SerialModel
 from trulens_eval.util import TP
-from trulens_eval.util import UNICODE_CHECK
-from trulens_eval.util import UNICODE_CLOCK
-from trulens_eval.util import UNICODE_YIELD
+from trulens_eval.util import WithClassInfo
+from trulens_eval.utils.text import UNICODE_CHECK
+from trulens_eval.utils.text import UNICODE_CLOCK
+from trulens_eval.utils.text import UNICODE_YIELD
 
 PROVIDER_CLASS_NAMES = ['OpenAI', 'Huggingface', 'Cohere']
 
-default_pass_fail_color_threshold = 0.5
-
 logger = logging.getLogger(__name__)
 
 
 def check_provider(cls_or_name: Union[Type, str]) -> None:
     if isinstance(cls_or_name, str):
         cls_name = cls_or_name
     else:
         cls_name = cls_or_name.__name__
 
     assert cls_name in PROVIDER_CLASS_NAMES, f"Unsupported provider class {cls_name}"
 
+# Signature of feedback implementations. Take in any number of arguments
+# and return either a single float or a float and a dictionary (of metadata).
+ImpCallable = Callable[..., Union[float, Tuple[float, Dict[str, Any]]]]
+
+# Signature of aggregation functions.
+AggCallable = Callable[[Iterable[float]], float]
 
 class Feedback(FeedbackDefinition):
     # Implementation, not serializable, note that FeedbackDefinition contains
     # `implementation` meant to serialize the below.
-    imp: Optional[Callable] = pydantic.Field(exclude=True)
+    imp: Optional[ImpCallable] = pydantic.Field(exclude=True)
 
     # Aggregator method for feedback functions that produce more than one
     # result.
-    agg: Optional[Callable] = pydantic.Field(exclude=True)
+    agg: Optional[AggCallable] = pydantic.Field(exclude=True)
 
     def __init__(
         self,
         imp: Optional[Callable] = None,
         agg: Optional[Callable] = None,
         **kwargs
     ):
         """
         A Feedback function container.
 
         Parameters:
         
         - imp: Optional[Callable] -- implementation of the feedback function.
+
+        - agg: Optional[Callable] -- aggregation function for producing a single
+          float for feedback implementations that are run more than once.
         """
 
         agg = agg or np.mean
 
+        # imp is the python function/method while implementation is a serialized
+        # json structure. Create the one that is missing based on the one that
+        # is provided:
+
         if imp is not None:
             # These are for serialization to/from json and for db storage.
-            kwargs['implementation'] = FunctionOrMethod.of_callable(
-                imp, loadable=True
-            )
+            if 'implementation' not in kwargs:
+                try:
+                    kwargs['implementation'] = FunctionOrMethod.of_callable(
+                        imp, loadable=True
+                    )
+                except ImportError as e:
+                    logger.warning(
+                        f"Feedback implementation {imp} cannot be serialized: {e}. "
+                        f"This may be ok unless you are using the deferred feedback mode."
+                    )
+
+                    kwargs['implementation'] = FunctionOrMethod.of_callable(
+                        imp, loadable=False
+                    )
 
         else:
             if "implementation" in kwargs:
-                imp: Callable = FunctionOrMethod.pick(
+                imp: ImpCallable = FunctionOrMethod.pick(
                     **(kwargs['implementation'])
                 ).load() if kwargs['implementation'] is not None else None
 
+        # Similarly with agg and aggregator.
         if agg is not None:
-            try:
-                # These are for serialization to/from json and for db storage.
-                kwargs['aggregator'] = FunctionOrMethod.of_callable(
-                    agg, loadable=True
-                )
-            except:
-                # User defined functions in script do not have a module so cannot be serialized
-                pass
+            if 'aggregator' not in kwargs:
+                try:
+                    # These are for serialization to/from json and for db storage.            
+                    kwargs['aggregator'] = FunctionOrMethod.of_callable(
+                        agg, loadable=True
+                    )
+                except:
+                    # User defined functions in script do not have a module so cannot be serialized
+                    pass
         else:
             if 'aggregator' in kwargs:
-                agg: Callable = FunctionOrMethod.pick(**(kwargs['aggregator'])
+                agg: AggCallable = FunctionOrMethod.pick(**(kwargs['aggregator'])
                                                      ).load()
 
         super().__init__(**kwargs)
 
         self.imp = imp
         self.agg = agg
 
@@ -506,17 +536,31 @@
             for argname in self.selectors.keys():
                 assert argname in sig.parameters, (
                     f"{argname} is not an argument to {self.imp.__name__}. "
                     f"Its arguments are {list(sig.parameters.keys())}."
                 )
 
     def on_input_output(self):
+        """
+        Specifies that the feedback implementation arguments are to be the main
+        app input and output in that order.
+
+        Returns a new Feedback object with the specification.
+        """
         return self.on_input().on_output()
 
     def on_default(self):
+        """
+        Specifies that one argument feedbacks should be evaluated on the main
+        app output and two argument feedbacks should be evaluates on main input
+        and main output in that order.
+
+        Returns a new Feedback object with this specification.
+        """
+
         ret = Feedback().parse_obj(self)
         ret._default_selectors()
         return ret
 
     def _print_guessed_selector(self, par_name, par_path):
         if par_path == Select.RecordCalls:
             alias_info = f" or `Select.RecordCalls`"
@@ -524,15 +568,16 @@
             alias_info = f" or `Select.RecordInput`"
         elif par_path == Select.RecordOutput:
             alias_info = f" or `Select.RecordOutput`"
         else:
             alias_info = ""
 
         print(
-            f"{UNICODE_CHECK} In {self.name}, input {par_name} will be set to {par_path}{alias_info} ."
+            f"{UNICODE_CHECK} In {self.name}, "
+            f"input {par_name} will be set to {par_path}{alias_info} ."
         )
 
     def _default_selectors(self):
         """
         Fill in default selectors for any remaining feedback function arguments.
         """
 
@@ -569,14 +614,19 @@
                 f"The feedback function has signature {sig}."
             )
 
         self.selectors = selectors
 
     @staticmethod
     def evaluate_deferred(tru: 'Tru') -> int:
+        """
+        Evaluates feedback functions that were specified to be deferred. Returns
+        an integer indicating how many evaluates were run.
+        """
+
         db = tru.db
 
         def prepare_feedback(row):
             record_json = row.record_json
             record = Record(**record_json)
 
             app_json = row.app_json
@@ -605,48 +655,59 @@
                 TP().runlater(prepare_feedback, row)
                 started_count += 1
 
             elif row.status in [FeedbackResultStatus.RUNNING]:
                 now = datetime.now().timestamp()
                 if now - row.last_ts > 30:
                     print(
-                        f"{UNICODE_YIELD} Feedback task last made progress over 30 seconds ago. Retrying: {feedback_ident}"
+                        f"{UNICODE_YIELD} Feedback task last made progress over 30 seconds ago. "
+                        f"Retrying: {feedback_ident}"
                     )
                     TP().runlater(prepare_feedback, row)
                     started_count += 1
 
                 else:
                     print(
-                        f"{UNICODE_CLOCK} Feedback task last made progress less than 30 seconds ago. Giving it more time: {feedback_ident}"
+                        f"{UNICODE_CLOCK} Feedback task last made progress less than 30 seconds ago. "
+                        f"Giving it more time: {feedback_ident}"
                     )
 
             elif row.status in [FeedbackResultStatus.FAILED]:
                 now = datetime.now().timestamp()
                 if now - row.last_ts > 60 * 5:
                     print(
-                        f"{UNICODE_YIELD} Feedback task last made progress over 5 minutes ago. Retrying: {feedback_ident}"
+                        f"{UNICODE_YIELD} Feedback task last made progress over 5 minutes ago. "
+                        f"Retrying: {feedback_ident}"
                     )
                     TP().runlater(prepare_feedback, row)
                     started_count += 1
 
                 else:
                     print(
-                        f"{UNICODE_CLOCK} Feedback task last made progress less than 5 minutes ago. Not touching it for now: {feedback_ident}"
+                        f"{UNICODE_CLOCK} Feedback task last made progress less than 5 minutes ago. "
+                        f"Not touching it for now: {feedback_ident}"
                     )
 
             elif row.status == FeedbackResultStatus.DONE:
                 pass
 
         return started_count
 
     def __call__(self, *args, **kwargs) -> Any:
         assert self.imp is not None, "Feedback definition needs an implementation to call."
         return self.imp(*args, **kwargs)
 
     def aggregate(self, func: Callable) -> 'Feedback':
+        """
+        Specify the aggregation function in case the selectors for this feedback
+        generate more than one value for implementation argument(s).
+
+        Returns a new Feedback object with the given aggregation function.
+        """
+
         return Feedback(imp=self.imp, selectors=self.selectors, agg=func)
 
     @staticmethod
     def of_feedback_definition(f: FeedbackDefinition):
         implementation = f.implementation
         aggregator = f.aggregator
 
@@ -749,25 +810,40 @@
         feedback_result = FeedbackResult(
             feedback_definition_id=self.feedback_definition_id,
             record_id=record.record_id,
             name=self.name
         )
 
         try:
+            # Total cost, will accumulate.
             cost = Cost()
-
+            
             for ins in self.extract_selection(app=app_json, record=record):
 
-                result_val, part_cost = Endpoint.track_all_costs_tally(
+                result_and_meta, part_cost = Endpoint.track_all_costs_tally(
                     lambda: self.imp(**ins)
                 )
                 cost += part_cost
+
+                if isinstance(result_and_meta, Tuple):
+                    # If output is a tuple of two, we assume it is the float and the metadata.
+                    assert len(result_and_meta) == 2, "Feedback functions must return either a single float or a float and a dictionary."
+                    result_val, meta = result_and_meta
+
+                    assert isinstance(meta, dict), f"Feedback metadata output must be a dictionary but was {type(call_meta)}."
+                else:
+                    # Otherwise it is just the float. We create empty metadata dict.
+                    result_val = result_and_meta
+                    meta = dict()
+
+                assert isinstance(result_val, float), f"Feedback function output must be a float but was {type(result_val)}."
+                    
                 result_vals.append(result_val)
 
-                feedback_call = FeedbackCall(args=ins, ret=result_val)
+                feedback_call = FeedbackCall(args=ins, ret=result_val, meta=meta)
                 feedback_calls.append(feedback_call)
 
             result_vals = np.array(result_vals)
             if len(result_vals) == 0:
                 logger.warning(
                     f"Feedback function {self.name} with aggregation {self.agg} had no inputs."
                 )
@@ -902,53 +978,66 @@
         if not matches:
             logger.warn(f"1-10 rating regex failed to match on: '{str_val}'")
             return -10  # so this will be reported as -1 after division by 10
 
     return int(matches.group())
 
 
-class Provider(SerialModel):
+class Provider(SerialModel, WithClassInfo):
 
     class Config:
         arbitrary_types_allowed = True
 
     endpoint: Optional[Endpoint]
 
+    def __init__(self, *args, **kwargs):
+        # for WithClassInfo:
+        kwargs['obj'] = self
+
+        super().__init__(*args, **kwargs)
+
 
 class OpenAI(Provider):
-    model_engine: str = "gpt-3.5-turbo"
+    model_engine: str
+
+    endpoint: Endpoint
 
-    # Exclude is important here so that pydantic doesn't try to
-    # serialize/deserialize the constant fixed endpoint we need.
-    endpoint: Endpoint = pydantic.Field(
-        default_factory=OpenAIEndpoint, exclude=True
-    )
+    def __init__(self, *args, endpoint = None, model_engine = "gpt-3.5-turbo", **kwargs):
+        # NOTE(piotrm): pydantic adds endpoint to the signature of this
+        # constructor if we don't include it explicitly, even though we set it
+        # down below. Adding it as None here as a temporary hack.
 
-    def __init__(self, **kwargs):
         """
         A set of OpenAI Feedback Functions.
 
         Parameters:
 
         - model_engine (str, optional): The specific model version. Defaults to
           "gpt-3.5-turbo".
+
+        - All other args/kwargs passed to OpenAIEndpoint constructor.
         """
 
+        # TODO: why was self_kwargs required here independently of kwargs?
+        self_kwargs = dict()
+        self_kwargs['model_engine'] = model_engine
+        self_kwargs['endpoint'] = OpenAIEndpoint(*args, **kwargs)
+
         super().__init__(
-            **kwargs
+            **self_kwargs
         )  # need to include pydantic.BaseModel.__init__
 
         set_openai_key()
 
     """
     def to_json(self) -> Dict:
         return Provider.to_json(self, model_engine=self.model_engine)
     """
 
-    def _create_chat_completition(self, *args, **kwargs):
+    def _create_chat_completion(self, *args, **kwargs):
         return openai.ChatCompletion.create(*args, **kwargs)
 
     def _moderation(self, text: str):
         return self.endpoint.run_me(
             lambda: openai.Moderation.create(input=text)
         )
 
@@ -1088,15 +1177,15 @@
 
         Returns:
             float: A value between 0 and 1. 0 being "not relevant" and 1 being
             "relevant".
         """
         return _re_1_10_rating(
             self.endpoint.run_me(
-                lambda: self._create_chat_completition(
+                lambda: self._create_chat_completion(
                     model=self.model_engine,
                     temperature=0.0,
                     messages=[
                         {
                             "role":
                                 "system",
                             "content":
@@ -1122,15 +1211,15 @@
 
         Returns:
             float: A value between 0 and 1. 0 being "not relevant" and 1 being
             "relevant".
         """
         return _re_1_10_rating(
             self.endpoint.run_me(
-                lambda: self._create_chat_completition(
+                lambda: self._create_chat_completion(
                     model=self.model_engine,
                     temperature=0.0,
                     messages=[
                         {
                             "role":
                                 "system",
                             "content":
@@ -1141,14 +1230,46 @@
                                 )
                         }
                     ]
                 )["choices"][0]["message"]["content"]
             )
         ) / 10
 
+    def sentiment(self, text: str) -> float:
+        """
+        Uses OpenAI's Chat Completion Model. A function that completes a
+        template to check the sentiment of some text.
+
+        Parameters:
+            text (str): A prompt to an agent. response (str): The agent's
+            response to the prompt.
+
+        Returns:
+            float: A value between 0 and 1. 0 being "negative sentiment" and 1
+            being "positive sentiment".
+        """
+
+        return _re_1_10_rating(
+            self.endpoint.run_me(
+                lambda: self._create_chat_completion(
+                    model=self.model_engine,
+                    temperature=0.0,
+                    messages=[
+                        {
+                            "role": "system",
+                            "content": feedback_prompts.SENTIMENT_SYSTEM_PROMPT
+                        }, {
+                            "role": "user",
+                            "content": text
+                        }
+                    ]
+                )["choices"][0]["message"]["content"]
+            )
+        )
+    
     def model_agreement(self, prompt: str, response: str) -> float:
         """
         Uses OpenAI's Chat GPT Model. A function that gives Chat GPT the same
         prompt and gets a response, encouraging truthfulness. A second template
         is given to Chat GPT with a prompt that the original response is
         correct, and measures whether previous Chat GPT's response is similar.
 
@@ -1156,71 +1277,132 @@
             prompt (str): A text prompt to an agent. response (str): The agent's
             response to the prompt.
 
         Returns:
             float: A value between 0 and 1. 0 being "not in agreement" and 1
             being "in agreement".
         """
-        oai_chat_response = OpenAI().endpoint.run_me(
-            lambda: self._create_chat_completition(
+        logger.warning("model_agreement has been deprecated. Use GroundTruthAgreement(ground_truth) instead.")
+        oai_chat_response = self.endpoint.run_me(
+            lambda: self._create_chat_completion(
                 model=self.model_engine,
                 temperature=0.0,
                 messages=[
                     {
                         "role": "system",
                         "content": feedback_prompts.CORRECT_SYSTEM_PROMPT
                     }, {
                         "role": "user",
                         "content": prompt
                     }
                 ]
             )["choices"][0]["message"]["content"]
         )
-        agreement_txt = _get_answer_agreement(
+        agreement_txt = self._get_answer_agreement(
             prompt, response, oai_chat_response, self.model_engine
         )
         return _re_1_10_rating(agreement_txt) / 10
 
-    def sentiment(self, text: str) -> float:
+    def _get_answer_agreement(self, prompt, response, check_response, model_engine="gpt-3.5-turbo"):
+        oai_chat_response = self.endpoint.run_me(
+            lambda: self._create_chat_completion(
+                model=model_engine,
+                temperature=0.0,
+                messages=[
+                    {
+                        "role":
+                            "system",
+                        "content":
+                            feedback_prompts.AGREEMENT_SYSTEM_PROMPT %
+                            (prompt, response)
+                    }, {
+                        "role": "user",
+                        "content": check_response
+                    }
+                ]
+            )["choices"][0]["message"]["content"]
+        )
+        return oai_chat_response
+
+
+
+class GroundTruthAgreement(SerialModel, WithClassInfo):
+    ground_truth: Union[List[str], FunctionOrMethod]
+    provider: OpenAI
+
+    ground_truth_imp: Optional[Callable] = pydantic.Field(exclude=True)
+
+    def __init__(self, ground_truth: Union[List[str], Callable, FunctionOrMethod], provider: OpenAI = OpenAI()):
+        if isinstance(ground_truth, List):
+            ground_truth_imp = None
+        elif isinstance(ground_truth, FunctionOrMethod):
+            ground_truth_imp = ground_truth.load()
+        elif isinstance(ground_truth, Callable):
+            ground_truth_imp = ground_truth
+            ground_truth = FunctionOrMethod.of_callable(ground_truth)
+        elif isinstance(ground_truth, Dict):
+            # Serialized FunctionOrMethod?
+            ground_truth = FunctionOrMethod.pick(**ground_truth)
+            ground_truth_imp = ground_truth.load()
+        else:
+            raise RuntimeError(f"Unhandled ground_truth type: {type(ground_truth)}.")
+
+        super().__init__(
+            ground_truth=ground_truth,
+            ground_truth_imp=ground_truth_imp,
+            provider=provider,
+            obj=self # for WithClassInfo
+        )
+
+    def _find_response(self, prompt: str) -> Optional[str]:
+        if self.ground_truth_imp is not None:
+            return self.ground_truth_imp(prompt)
+
+        responses = [qr["response"] for qr in self.ground_truth if qr["query"] == prompt]
+        if responses:
+            return responses[0]
+        else:
+            return None
+
+    def agreement_measure(self, prompt: str, response: str) -> Union[float, Tuple[float, Dict[str, str]]]:
         """
-        Uses OpenAI's Chat Completion Model. A function that completes a
-        template to check the sentiment of some text.
+        Uses OpenAI's Chat GPT Model. A function that that measures
+        similarity to ground truth. A second template is given to Chat GPT
+        with a prompt that the original response is correct, and measures
+        whether previous Chat GPT's response is similar.
 
         Parameters:
-            text (str): A prompt to an agent. response (str): The agent's
-            response to the prompt.
+            prompt (str): A text prompt to an agent. response (str): The
+            agent's response to the prompt.
 
         Returns:
-            float: A value between 0 and 1. 0 being "negative sentiment" and 1
-            being "positive sentiment".
-        """
-
-        return _re_1_10_rating(
-            self.endpoint.run_me(
-                lambda: self._create_chat_completition(
-                    model=self.model_engine,
-                    temperature=0.5,
-                    messages=[
-                        {
-                            "role": "system",
-                            "content": feedback_prompts.SENTIMENT_SYSTEM_PROMPT
-                        }, {
-                            "role": "user",
-                            "content": text
-                        }
-                    ]
-                )["choices"][0]["message"]["content"]
+            - float: A value between 0 and 1. 0 being "not in agreement" and 1
+                being "in agreement".
+            - dict: with key 'ground_truth_response'
+        """
+        ground_truth_response = self._find_response(prompt)
+        if ground_truth_response:
+            agreement_txt = self.provider._get_answer_agreement(
+                prompt, response, ground_truth_response
             )
-        )
+            ret = _re_1_10_rating(agreement_txt) / 10, dict(ground_truth_response=ground_truth_response)
+        else:
+            ret = np.nan
+        return ret
+
 
 
 class AzureOpenAI(OpenAI):
     deployment_id: str
 
-    def __init__(self, **kwargs):
+    def __init__(self, endpoint=None, **kwargs):
+        # NOTE(piotrm): pydantic adds endpoint to the signature of this
+        # constructor if we don't include it explicitly, even though we set it
+        # down below. Adding it as None here as a temporary hack.
+
         """
         Wrapper to use Azure OpenAI. Please export the following env variables
 
         - OPENAI_API_BASE
         - OPENAI_API_VERSION
         - OPENAI_API_KEY
 
@@ -1236,72 +1418,49 @@
         )  # need to include pydantic.BaseModel.__init__
 
         set_openai_key()
         openai.api_type = "azure"
         openai.api_base = os.getenv("OPENAI_API_BASE")
         openai.api_version = os.getenv("OPENAI_API_VERSION")
 
-    def _create_chat_completition(self, *args, **kwargs):
+    def _create_chat_completion(self, *args, **kwargs):
         """
         We need to pass `engine`
         """
-        return super()._create_chat_completition(
+        return super()._create_chat_completion(
             *args, deployment_id=self.deployment_id, **kwargs
         )
 
-
-def _get_answer_agreement(prompt, response, check_response, model_engine):
-    print("DEBUG")
-    print(feedback_prompts.AGREEMENT_SYSTEM_PROMPT % (prompt, response))
-    print("MODEL ANSWER")
-    print(check_response)
-    oai_chat_response = OpenAI().endpoint.run_me(
-        lambda: openai.ChatCompletion.create(
-            model=model_engine,
-            temperature=0.5,
-            messages=[
-                {
-                    "role":
-                        "system",
-                    "content":
-                        feedback_prompts.AGREEMENT_SYSTEM_PROMPT %
-                        (prompt, response)
-                }, {
-                    "role": "user",
-                    "content": check_response
-                }
-            ]
-        )["choices"][0]["message"]["content"]
-    )
-    return oai_chat_response
-
-
 # Cannot put these inside Huggingface since it interferes with pydantic.BaseModel.
 HUGS_SENTIMENT_API_URL = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment"
 HUGS_TOXIC_API_URL = "https://api-inference.huggingface.co/models/martin-ha/toxic-comment-model"
 HUGS_CHAT_API_URL = "https://api-inference.huggingface.co/models/facebook/blenderbot-3B"
 HUGS_LANGUAGE_API_URL = "https://api-inference.huggingface.co/models/papluca/xlm-roberta-base-language-detection"
 
 
 class Huggingface(Provider):
 
-    # Exclude is important here so that pydantic doesn't try to
-    # serialize/deserialize the constant fixed endpoint we need.
-    endpoint: Endpoint = pydantic.Field(
-        default_factory=HuggingfaceEndpoint, exclude=True
-    )
+    endpoint: Endpoint
+    
+    def __init__(self, endpoint=None, **kwargs):
+        # NOTE(piotrm): pydantic adds endpoint to the signature of this
+        # constructor if we don't include it explicitly, even though we set it
+        # down below. Adding it as None here as a temporary hack.
 
-    def __init__(self, **kwargs):
         """
-        A set of Huggingface Feedback Functions. Utilizes huggingface
-        api-inference.
+        A set of Huggingface Feedback Functions.
+
+        All args/kwargs passed to HuggingfaceEndpoint constructor.
         """
 
+        self_kwargs = dict()
+        self_kwargs['endpoint'] = HuggingfaceEndpoint(**kwargs)
+
         super().__init__(
-            **kwargs
+            **self_kwargs
         )  # need to include pydantic.BaseModel.__init__
 
     def language_match(self, text1: str, text2: str) -> float:
         """
         Uses Huggingface's papluca/xlm-roberta-base-language-detection model. A
         function that uses language detection on `text1` and `text2` and
         calculates the probit difference on the language detected on text1. The
@@ -1341,15 +1500,15 @@
         langs = list(scores1.keys())
         prob1 = np.array([scores1[k] for k in langs])
         prob2 = np.array([scores2[k] for k in langs])
         diff = prob1 - prob2
 
         l1 = 1.0 - (np.linalg.norm(diff, ord=1)) / 2.0
 
-        return l1
+        return l1, dict(text1_scores=scores1, text2_scores=scores2)
 
     def positive_sentiment(self, text: str) -> float:
         """
         Uses Huggingface's cardiffnlp/twitter-roberta-base-sentiment model. A
         function that uses a sentiment classifier on `text`.
         
         Parameters:
@@ -1391,24 +1550,27 @@
         )
 
         for label in hf_response:
             if label['label'] == 'toxic':
                 return label['score']
 
 
-# cohere
 class Cohere(Provider):
     model_engine: str = "large"
 
-    def __init__(self, model_engine='large'):
-        super().__init__()  # need to include pydantic.BaseModel.__init__
+    def __init__(self, model_engine='large', endpoint=None, **kwargs):
+        # NOTE(piotrm): pydantic adds endpoint to the signature of this
+        # constructor if we don't include it explicitly, even though we set it
+        # down below. Adding it as None here as a temporary hack.
 
-        Cohere().endpoint = Endpoint(name="cohere")
-        self.model_engine = model_engine
+        kwargs['endpoint'] = Endpoint(name="cohere")
+        kwargs['model_engine'] = model_engine
 
+        super().__init__(**kwargs)  # need to include pydantic.BaseModel.__init__
+        
     def sentiment(
         self,
         text,
     ):
         return int(
             Cohere().endpoint.run_me(
                 lambda: get_cohere_agent().classify(
```

## trulens_eval/feedback_prompts.py

```diff
@@ -1,29 +1,72 @@
 from cohere.responses.classify import Example
 
-QS_RELEVANCE = """You are a RELEVANCE classifier; providing the relevance of the given STATEMENT to the given QUESTION.
-Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.
-Never elaborate.
+QS_RELEVANCE = """You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.
+Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant. 
+
+A few additional scoring guidelines:
+
+- Long STATEMENTS should score equally well as short STATEMENTS.
+
+- RELEVANCE score should increase as the STATEMENT provides more RELEVANT context to the QUESTION.
+
+- RELEVANCE score should increase as the STATEMENT provides RELEVANT context to more parts of the QUESTION.
+
+- STATEMENT that is RELEVANT to some of the QUESTION should score of 2, 3 or 4. Higher score indicates more RELEVANCE.
+
+- STATEMENT that is RELEVANT to most of the QUESTION should get a score of 5, 6, 7 or 8. Higher score indicates more RELEVANCE.
+
+- STATEMENT that is RELEVANT to the entire QUESTION should get a score of 9 or 10. Higher score indicates more RELEVANCE.
+
+- STATEMENT must be relevant and helpful for answering the entire QUESTION to get a score of 10.
+
+- Answers that intentionally do not answer the question, such as 'I don't know', should also be counted as the most relevant.
+
+- Never elaborate.
 
 QUESTION: {question}
 
 STATEMENT: {statement}
 
 RELEVANCE: """
 
-PR_RELEVANCE = """
-You are a relevance classifier, providing the relevance of a given response to the given prompt.
-Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.
-Never elaborate.
+PR_RELEVANCE = """You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.
+Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant. 
+
+A few additional scoring guidelines:
+
+- Long RESPONSES should score equally well as short RESPONSES.
 
-Prompt: {prompt}
+- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should also be counted as the most RELEVANT.
 
-Response: {response}
+- RESPONSE must be relevant to the entire PROMPT to get a score of 10.
 
-Relevance: """
+- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.
+
+- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 1.
+
+- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.
+
+- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.
+
+- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.
+
+- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.
+
+- RESPONSE that confidently FALSE should get a score of 1.
+
+- RESPONSE that is only seemingly RELEVANT should get a score of 1.
+
+- Never elaborate.
+
+PROMPT: {prompt}
+
+RESPONSE: {response}
+
+RELEVANCE: """
 
 SENTIMENT_SYSTEM_PROMPT = f"Please classify the sentiment of the following text as 1 if positive or 0 if not positive. Respond with only a '1' or '0', nothing more."
 RELEVANCE_SYSTEM_PROMPT = f"You are a relevance classifier, providing the relevance of a given response to a particular prompt. \n"
 "Provide all responses only as a number from 1 to 10 where 1 is the least relevant and 10 is the most. Always respond with an integer between 1 and 10. \n"
 "Never elaborate. The prompt is: "
 RELEVANCE_CONTENT_PROMPT = f"For that prompt, how relevant is this response on the scale between 1 and 10: "
```

## trulens_eval/keys.py

```diff
@@ -1,131 +1,437 @@
 """
-Read secrets from .env for exporting to python scripts. Usage:
+# API keys and configuration 
+
+## Setting keys
+
+To check whether appropriate api keys have been set:
+
+```python 
+from trulens_eval.keys import check_keys
+
+check_keys(
+    "OPENAI_API_KEY",
+    "HUGGINGFACE_API_KEY"
+)
+```
+
+Alternatively you can set using `check_or_set_keys`:
+
+```python 
+from trulens_eval.keys import check_or_set_keys
+
+check_or_set_keys(
+    OPENAI_API_KEY="to fill in", 
+    HUGGINGFACE_API_KEY="to fill in"
+)
+```
+
+This line checks that you have the requisite api keys set before continuing the
+notebook. They do not need to be provided, however, right on this line. There
+are several ways to make sure this check passes:
+
+- *Explicit* -- Explicitly provide key values to `check_keys`.
+
+- *Python* -- Define variables before this check like this:
 
 ```python
-from keys import *
+OPENAI_API_KEY="something"
 ```
 
-Will get you access to all of the vars defined in .env in wherever you put that
-import statement.
+- *Environment* -- Set them in your environment variable. They should be visible when you execute:
+
+```python
+import os
+print(os.environ)
+```
+
+- *.env* -- Set them in a .env file in the same folder as the example notebook or one of
+  its parent folders. An example of a .env file is found in
+  `trulens_eval/trulens_eval/env.example` .
+
+- *3rd party* -- For some keys, set them as arguments to the 3rd-party endpoint class. For
+  example, with `openai`, do this ahead of the `check_keys` check:
+
+```python
+import openai
+openai.api_key = "something"
+```
+
+- *Endpoint class* For some keys, set them as arguments to trulens_eval endpoint class that
+  manages the endpoint. For example, with `openai`, do this ahead of the
+  `check_keys` check:
+
+```python
+from trulens_eval.provider_apis import OpenAIEndpoint
+openai_endpoint = OpenAIEndpoint(api_key="something")
+```
+
+- *Provider class* For some keys, set them as arguments to trulens_eval feedback
+  collection ("provider") class that makes use of the relevant endpoint. For
+  example, with `openai`, do this ahead of the `check_keys` check:
+
+```python
+from trulens_eval.feedback import OpenAI
+openai_feedbacks = OpenAI(api_key="something")
+```
+
+In the last two cases, please note that the settings are global. Even if you
+create multiple OpenAI or OpenAIEndpoint objects, they will share the
+configuration of keys (and other openai attributes).
+
+## Other API attributes
+
+Some providers may require additional configuration attributes beyond api key.
+For example, `openai` usage via azure require special keys. To set those, you
+should use the 3rd party class method of configuration. For example with
+`openai`:
+
+```python
+import openai
+
+openai.api_type = "azure"
+openai.api_key = "..."
+openai.api_base = "https://example-endpoint.openai.azure.com"
+openai.api_version = "2023-05-15"  # subject to change
+# See https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/switching-endpoints .
+```
+
+Our example notebooks will only check that the api_key is set but will make use
+of the configured openai object as needed to compute feedback.
 """
 
+from collections import defaultdict
 import logging
 import os
 from pathlib import Path
+import re
+from typing import Any, Optional, Set, Tuple, Union, Dict
 
 import cohere
 import dotenv
 
-from trulens_eval.util import caller_frame
-from trulens_eval.util import UNICODE_CHECK
-from trulens_eval.util import UNICODE_STOP
+from trulens_eval.utils.python import caller_frame
+from trulens_eval.utils.text import UNICODE_CHECK
+from trulens_eval.utils.text import UNICODE_STOP
 
 logger = logging.getLogger(__name__)
 
+# Keep track of values that should not be shown in UI (or added to DB). This set
+# is only used for cases where the name/key for a field is not useful to
+# determine whether it should be redacted.
+values_to_redact: Set[str] = set()
+
+# Regex of keys (into dict/json) that should be redacted.
+RE_KEY_TO_REDACT: re.Pattern = re.compile(
+    '|'.join(
+        [
+            r'api_key',
+            # Covers OpenAI, Cohere, Anthropic class key 'api_key'
+            r'.+_api_key',
+            # Covers langchain llm attributes for keys such as 'openai_api_key'.
+
+            # r'token',
+            # Would cover bard unofficial api field "token" but this is a
+            # bit too general of a key; TODO: need another solution to redact.
+            r'.+_API_KEY',
+            # Covers env vars ending in "_API_KEY", including openai, cohere, anthropic,
+            # bard
+            r'KAGGLE_KEY',
+            r'SLACK_(TOKEN|SIGNING_SECRET)',
+            # Covers slack-related keys.
+        ]
+    )
+)
+
+# Env vars not covered as they are assumed non-sensitive:
+# - PINECONE_ENV, e.g. "us-west1-gcp-free"
+# - KAGGLE_USER
+
+# Keys not covered that might be sensitive:
+# - "token" - i.e. bard-api Bard.token, slack api's -- name collision with
+#   "token" as in the basic building block of text.
+
+# TODO: Some method for letting users add more things to redact.
+
+# The replacement value for redacted values.
+REDACTED_VALUE = "__tru_redacted"
 
-def get_config():
+# Treat these value as not valid keys. Use any as a templates to suggest a user
+# fills in the key.
+TEMPLATE_VALUES = set(["to fill in"])
+
+global cohere_agent
+cohere_agent = None
+
+
+def should_redact_key(k: Optional[str]) -> bool:
+    return isinstance(k, str) and RE_KEY_TO_REDACT.fullmatch(k)
+
+
+def should_redact_value(v: Union[Any, str]) -> bool:
+    return isinstance(v, str) and v in values_to_redact
+
+
+def redact_value(v: Union[str, Any],
+                 k: Optional[str] = None) -> Union[str, Any]:
+    """
+    Determine whether the given value `v` should be redacted and redact it if
+    so. If its key `k` (in a dict/json-like) is given, uses the key name to
+    determine whether redaction is appropriate. If key `k` is not given, only
+    redacts if `v` is a string and identical to one of the keys ingested using
+    `setup_keys`.
+    """
+
+    if should_redact_key(k) or should_redact_value(v):
+        return REDACTED_VALUE
+    else:
+        return v
+
+
+def get_config_file() -> Path:
+    """
+    Looks for a .env file in current folder or its parents. Returns Path of
+    found .env or None if not found.
+    """
     for path in [Path().cwd(), *Path.cwd().parents]:
         file = path / ".env"
         if file.exists():
-            # print(f"Using {file}")
-
             return file
 
     return None
 
 
-config_file = get_config()
-if config_file is None:
-    logger.warning(
-        f"No .env found in {Path.cwd()} or its parents. "
-        "You may need to specify secret keys in another manner."
-    )
-
-else:
-    config = dotenv.dotenv_values(config_file)
-
-    for k, v in config.items():
-        # print(f"{config_file}: {k}")
-        globals()[k] = v
+def get_config() -> Tuple[Path, dict]:
+    config_file = get_config_file()
+    if config_file is None:
+        logger.warning(
+            f"No .env found in {Path.cwd()} or its parents. "
+            "You may need to specify secret keys in another manner."
+        )
+        return None, None
+    else:
+        return config_file, dotenv.dotenv_values(config_file)
+
+        # Put value in redaction list.
+        values_to_redact.add(v)
 
-        # set them into environment as well
-        os.environ[k] = v
 
+def set_openai_key() -> None:
+    """
+    Sets the openai class attribute `api_key` to its value from the
+    OPENAI_API_KEY env var.
+    """
 
-def set_openai_key():
     if 'OPENAI_API_KEY' in os.environ:
         import openai
         openai.api_key = os.environ["OPENAI_API_KEY"]
 
 
-global cohere_agent
-cohere_agent = None
-
+def get_cohere_agent() -> cohere.Client:
+    """
+    Gete a singleton cohere agent. Sets its api key from env var COHERE_API_KEY.
+    """
 
-def get_cohere_agent():
     global cohere_agent
     if cohere_agent is None:
-        cohere.api_key = os.environ['COHERE_API_KEY']
+        cohere.api_key = os.environ['CO_API_KEY']
         cohere_agent = cohere.Client(cohere.api_key)
     return cohere_agent
 
 
-def get_huggingface_headers():
+def get_huggingface_headers() -> Dict[str, str]:
     HUGGINGFACE_HEADERS = {
         "Authorization": f"Bearer {os.environ['HUGGINGFACE_API_KEY']}"
     }
     return HUGGINGFACE_HEADERS
 
 
-def setup_keys(**kwargs):
-    global config_file
+def _value_is_set(v: str) -> bool:
+    return not (v is None or v in TEMPLATE_VALUES or v == "")
 
-    config_file = get_config()
-    if config_file is None:
-        logger.warning(
-            f"No .env found in {Path.cwd()} or its parents. "
-            "You may need to specify secret keys in another manner."
-        )
 
-    to_global = dict()
+class ApiKeyError(RuntimeError):
 
-    globs = caller_frame(offset=1).f_globals
+    def __init__(self, *args, key: str, msg: str = ""):
+        super().__init__(msg, *args)
+        self.key = key
+        self.msg = msg
 
-    for k, v in kwargs.items():
-        if v is not None and "fill" not in v.lower():
-            to_global[k] = v
-            print(f"{UNICODE_CHECK} Key {k} set explicitly.")
-            continue
-
-        if k in globs:
-            print(f"{UNICODE_CHECK} Key {k} was already set.")
-            continue
-
-        if k in os.environ:
-            v = os.environ[k]
-            to_global[k] = v
-            print(f"{UNICODE_CHECK} Key {k} set from environment.")
-            continue
-
-        if config_file is not None:
-            if k in config:
-                v = config[k]
-                print(f"{UNICODE_CHECK} Key {k} set from {config_file} .")
-                to_global[k] = v
-                continue
-
-        if "fill" in v:
-            raise RuntimeError(
-                f"""{UNICODE_STOP} Key {k} needs to be set; please provide it in one of these ways:
-- in a variable {k} prior to this check, 
-- in your variable environment, 
-- in a .env file in {Path.cwd()} or its parents, or 
-- explicitly passed to `setup_keys`.
+
+def _check_key(k: str, v: str = None) -> None:
+    """
+    Check that the given `k` is an env var with a value that indicates a valid
+    api key or secret.  If `v` is provided, checks that instead. If value
+    indicates the key is not set, raises an informative error telling the user
+    options on how to set that key.
+    """
+
+    v = v or os.environ.get(k)
+
+    if not _value_is_set(v):
+        msg = f"""Key {k} needs to be set; please provide it in one of these ways:
+
+  - in a variable {k} prior to this check, 
+  - in your variable environment, 
+  - in a .env file in {Path.cwd()} or its parents,
+  - explicitly passed to function `check_or_set_keys` of `trulens_eval.keys`,
+  - passed to the endpoint or feedback collection constructor that needs it (`trulens_eval.provider_apis.OpenAIEndpoint`, etc.), or
+  - set in api utility class that expects it (i.e. `openai`, etc.).
+
+For the last two options, the name of the argument may differ from {k} (i.e. `openai.api_key` for `OPENAI_API_KEY`).
 """
+        print(f"{UNICODE_STOP} {msg}")
+        raise ApiKeyError(key=k, msg=msg)
+
+
+def _relative_path(path: Path, relative_to: Path) -> str:
+    """
+    Get the path `path` relative to path `relative_to` even if `relative_to` is
+    not a prefix of `path`. Iteratively takes the parent of `relative_to` in
+    that case until it becomes a prefix. Each parent is indicated by '..'.
+    """
+
+    parents = 0
+
+    while True:
+        try:
+            return "".join(["../"] * parents
+                          ) + str(path.relative_to(relative_to))
+        except Exception:
+            parents += 1
+            relative_to = relative_to.parent
+
+
+def _collect_keys(*args: Tuple[str], **kwargs: Dict[str, str]) -> Dict[str, str]:
+    """
+    Collect values for keys from all of the currently supported sources. This includes:
+
+    - Using env variables.
+
+    - Using python variables.
+
+    - Explicitly passed to `check_or_set_keys`.
+
+    - Using vars defined in a .env file in current folder or one of its parents.
+
+    - Using 3rd party class attributes (i.e. OpenAI.api_key). This one requires the
+      user to initialize our Endpoint class for that 3rd party api.
+
+    - With initialization of trulens_eval Endpoint class that handles a 3rd party api.
+    """
+
+    ret = dict()
+
+    config_file, config = get_config()
+
+    globs = caller_frame(offset=2).f_globals
+
+    for k in list(args) + list(kwargs.keys()):
+        valid_values = set()
+        valid_sources = defaultdict(list)
+
+        # Env vars. NOTE: Endpoint classes copy over relevant keys from 3rd party
+        # classes (or provided explicitly to them) to var env.
+        temp_v = os.environ.get(k)
+        if _value_is_set(temp_v):
+            valid_sources[temp_v].append("environment")
+            valid_values.add(temp_v)
+
+        # Explicit.
+        temp_v = kwargs.get(k)
+        if _value_is_set(temp_v):
+            valid_sources[temp_v].append(
+                f"explicit value to `check_or_set_keys`"
+            )
+            valid_values.add(temp_v)
+
+        # .env vars.
+        if config is not None:
+            temp_v = config.get(k)
+            if _value_is_set(temp_v):
+                valid_sources[temp_v].append(f".env file at {config_file}")
+                valid_values.add(temp_v)
+
+        # Globals of caller.
+        temp_v = globs.get(k)
+        if _value_is_set(temp_v):
+            valid_sources[temp_v].append(f"python variable")
+            valid_values.add(temp_v)
+
+        if len(valid_values) == 0:
+            ret[k] = None
+
+        elif len(valid_values) > 1:
+            warning = f"More than one different value for key {k} has been found:\n\t"
+            warning += "\n\t".join(
+                f"""value ending in {v[-1]} in {' and '.join(valid_sources[v])}"""
+                for v in valid_values
+            )
+            warning += f"\nUsing one arbitrarily."
+            logger.warning(warning)
+
+            ret[k] = list(valid_values)[0]
+        else:
+            v = list(valid_values)[0]
+            print(
+                f"{UNICODE_CHECK} Key {k} set from {valid_sources[v][0]}" + (
+                    ' (same value found in ' +
+                    (' and '.join(valid_sources[v][1:])) +
+                    ')' if len(valid_sources[v]) > 1 else ''
+                ) + "."
             )
 
-    for k, v in to_global.items():
-        globs[k] = v
+            ret[k] = v
+
+    return ret
+
+
+def check_keys(*keys: Tuple[str]) -> None:
+    """
+    Check that all keys named in `*args` are set as env vars. Will fail with a
+    message on how to set missing key if one is missing. If all are provided
+    somewhere, they will be set in the env var as the canonical location where
+    we should expect them subsequently. Example:
+
+    ```python 
+    from trulens_eval.keys import check_keys
+
+    check_keys(
+        "OPENAI_API_KEY",
+        "HUGGINGFACE_API_KEY"
+    )
+    ```
+    """
+
+    kvals = _collect_keys(*keys)
+    for k in keys:
+        v = kvals.get(k)
+        _check_key(k, v=v)
+        values_to_redact.add(v)
         os.environ[k] = v
 
     set_openai_key()
+
+
+def check_or_set_keys(*args: Tuple[str], **kwargs: Dict[str, str]) -> None:
+    """
+    Check various sources of api configuration values like secret keys and set
+    env variables for each of them. We use env variables as the canonical
+    storage of these keys, regardless of how they were specified. Values can
+    also be specified explicitly to this method. Example:
+
+    ```python 
+    from trulens_eval.keys import check_or_set_keys
+
+    check_or_set_keys(
+        OPENAI_API_KEY="to fill in", 
+        HUGGINGFACE_API_KEY="to fill in"
+    )
+    ```
+    """
+
+    kvals = _collect_keys(*args, **kwargs)
+    for k in list(args) + list(kwargs.keys()):
+        v = kvals.get(k)
+        _check_key(k, v=v)
+        values_to_redact.add(v)
+        os.environ[k] = v
```

## trulens_eval/provider_apis.py

```diff
@@ -5,22 +5,25 @@
 from queue import Queue
 from threading import Thread
 from time import sleep
 from types import ModuleType
 from typing import (
     Any, Callable, Dict, Optional, Sequence, Tuple, Type, TypeVar
 )
+from pprint import PrettyPrinter
 
 from langchain.callbacks.openai_info import OpenAICallbackHandler
 from langchain.schema import LLMResult
 import pydantic
 import requests
 
 from trulens_eval.keys import get_huggingface_headers
 from trulens_eval.schema import Cost
+from trulens_eval.keys import _check_key
+from trulens_eval.utils.text import UNICODE_CHECK
 from trulens_eval.util import get_local_in_call_stack
 from trulens_eval.util import JSON
 from trulens_eval.util import SerialModel
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import WithClassInfo
 
 logger = logging.getLogger(__name__)
@@ -524,15 +527,15 @@
 
 
 class OpenAIEndpoint(Endpoint, WithClassInfo):
     """
     OpenAI endpoint. Instruments "create" methods in openai.* classes.
     """
 
-    def __new__(cls):
+    def __new__(cls, *args, **kwargs):
         return super(Endpoint, cls).__new__(cls, name="openai")
 
     def handle_wrapped_call(
         self, func: Callable, bindings: inspect.BoundArguments, response: Any,
         callback: Optional[EndpointCallback]
     ) -> None:
 
@@ -573,36 +576,75 @@
         if not counted_something:
             logger.warning(
                 f"Unregonized openai response format. It did not have usage information nor categories:\n"
                 + pp.pformat(response)
             )
 
     def __init__(self, *args, **kwargs):
+        # If any of these keys are in kwargs, copy over its value to the env
+        # variable named as the respective value in this dict. If value is None,
+        # don't copy to env. Regardless of env, set all of these as attributes
+        # to openai.
+
+        # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/switching-endpoints
+        CONF_CLONE = dict(
+            api_key="OPENAI_API_KEY",
+            organization=None,
+            api_type=None,
+            api_base=None,
+            api_version=None
+        )
+
+        import openai
+        import os
+
+        for k, v in CONF_CLONE.items():
+            if k in kwargs:
+                print(f"{UNICODE_CHECK} Setting openai.{k} explicitly.")
+                setattr(openai, k, kwargs[k])
+
+                if v is not None:
+                    print(f"{UNICODE_CHECK} Env. var. {v} set explicitly.")
+                    os.environ[v] = kwargs[k]
+            else:
+                if v is not None:
+                    # If no value were explicitly set, check if the user set up openai
+                    # attributes themselves and if so, copy over the ones we use via
+                    # environment vars, to its respective env var.
+
+                    attr_val = getattr(openai, k)
+                    if attr_val is not None and attr_val != os.environ.get(v):
+                        print(f"{UNICODE_CHECK} Env. var. {v} set from openai.{k} .")
+                        os.environ[v] = attr_val
+
+        # Will fail if key not set:
+        _check_key("OPENAI_API_KEY")
+
         if hasattr(self, "name"):
             # Already created with SingletonPerName mechanism
             return
 
         kwargs['name'] = "openai"
         kwargs['callback_class'] = OpenAICallback
 
         # for WithClassInfo:
         kwargs['obj'] = self
 
         super().__init__(*args, **kwargs)
 
-        import openai
         self._instrument_module_members(openai, "create")
 
 
 class HuggingfaceEndpoint(Endpoint, WithClassInfo):
     """
-    OpenAI endpoint. Instruments "create" methodsin openai.* classes.
+    Huggingface. Instruments the requests.post method for requests to
+    "https://api-inference.huggingface.co".
     """
 
-    def __new__(cls):
+    def __new__(cls, *args, **kwargs):
         return super(Endpoint, cls).__new__(cls, name="huggingface")
 
     def handle_wrapped_call(
         self, func: Callable, bindings: inspect.BoundArguments,
         response: requests.Response, callback: Optional[EndpointCallback]
     ) -> None:
         # Call here can only be requests.post .
@@ -620,14 +662,17 @@
 
         self.global_callback.handle_classification(response=response)
 
         if callback is not None:
             callback.handle_classification(response=response)
 
     def __init__(self, *args, **kwargs):
+        # Will fail if key not set:
+        _check_key("HUGGINGFACE_API_KEY")
+
         if hasattr(self, "name"):
             # Already created with SingletonPerName mechanism
             return
 
         kwargs['name'] = "huggingface"
         kwargs['post_headers'] = get_huggingface_headers()
         kwargs['callback_class'] = HuggingfaceCallback
```

## trulens_eval/schema.py

```diff
@@ -277,14 +277,18 @@
     DONE = "done"
 
 
 class FeedbackCall(SerialModel):
     args: Dict[str, str]
     ret: float
 
+    # New in 0.6.0: Any additional data a feedback function returns to display
+    # alongside its float result.
+    meta: Dict[str, Any] = pydantic.Field(default_factory=dict)
+
 
 class FeedbackResult(SerialModel):
     feedback_result_id: FeedbackResultID
 
     record_id: RecordID
 
     feedback_definition_id: Optional[FeedbackDefinitionID] = None
@@ -297,14 +301,15 @@
     cost: Cost = pydantic.Field(default_factory=Cost)
 
     name: str
 
     calls: Sequence[FeedbackCall] = []
     result: Optional[
         float] = None  # final result, potentially aggregating multiple calls
+
     error: Optional[str] = None  # if there was an error
 
     def __init__(
         self, feedback_result_id: Optional[FeedbackResultID] = None, **kwargs
     ):
 
         super().__init__(feedback_result_id="temporary", **kwargs)
```

## trulens_eval/tru.py

```diff
@@ -15,16 +15,16 @@
 from trulens_eval.db import LocalSQLite
 from trulens_eval.feedback import Feedback
 from trulens_eval.schema import AppDefinition
 from trulens_eval.schema import FeedbackResult
 from trulens_eval.schema import Record
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import TP
-from trulens_eval.util import UNICODE_CHECK
-from trulens_eval.util import UNICODE_YIELD
+from trulens_eval.utils.text import UNICODE_CHECK
+from trulens_eval.utils.text import UNICODE_YIELD
 from trulens_eval.utils.notebook_utils import is_notebook
 from trulens_eval.utils.notebook_utils import setup_widget_stdout_stderr
 
 logger = logging.getLogger(__name__)
 
 # How long to wait (seconds) for streamlit to print out url when starting the
 # dashboard.
```

## trulens_eval/tru_llama.py

```diff
@@ -48,16 +48,16 @@
             llama_index.indices.query.base.BaseQueryEngine,
             llama_index.indices.base_retriever.BaseRetriever,
             llama_index.indices.base.BaseIndex,
             llama_index.chat_engine.types.BaseChatEngine,
             llama_index.prompts.base.Prompt,
             # llama_index.prompts.prompt_type.PromptType, # enum
             llama_index.question_gen.types.BaseQuestionGenerator,
-            llama_index.indices.query.response_synthesis.ResponseSynthesizer,
-            llama_index.indices.response.refine.Refine,
+            llama_index.response_synthesizers.base.BaseSynthesizer,
+            llama_index.response_synthesizers.refine.Refine,
             llama_index.llm_predictor.LLMPredictor,
             llama_index.llm_predictor.base.LLMMetadata,
             llama_index.llm_predictor.base.BaseLLMPredictor,
             llama_index.vector_stores.types.VectorStore,
             llama_index.question_gen.llm_generators.BaseQuestionGenerator,
             llama_index.indices.service_context.ServiceContext,
             llama_index.indices.prompt_helper.PromptHelper,
@@ -66,16 +66,17 @@
         }.union(LangChainInstrument.Default.CLASSES())
 
         # Instrument only methods with these names and of these classes. Ok to
         # include llama_index inside methods.
         METHODS = dict_set_with(
             {
                 "get_response":
-                    lambda o:
-                    isinstance(o, llama_index.indices.response.refine.Refine),
+                    lambda o: isinstance(
+                        o, llama_index.response_synthesizers.refine.Refine
+                    ),
                 "predict":
                     lambda o: isinstance(
                         o, llama_index.llm_predictor.base.BaseLLMPredictor
                     ),
                 "query":
                     lambda o: isinstance(
                         o, llama_index.indices.query.base.BaseQueryEngine
```

## trulens_eval/util.py

```diff
@@ -1,8 +1,12 @@
 """
+TODO: This file got big. Split off pieces into the utils folder. Already split some:
+- utils/text.py -- text outputs/ui related utilities and constants
+- utils/python.py -- core python-related utilities
+
 # Utilities.
 
 ## Serialization of Python objects
 
 For deferred feedback evaluation, we need to serialize and deserialize python
 functions/methods. We feature several storage classes to accomplish this:
 
@@ -35,42 +39,37 @@
 from multiprocessing.pool import AsyncResult
 from multiprocessing.pool import ThreadPool
 from pathlib import Path
 from pprint import PrettyPrinter
 from queue import Queue
 from time import sleep
 from types import ModuleType
-from typing import (
-    Any, Callable, Dict, Hashable, Iterable, List, Optional, Sequence, Set,
-    Tuple, TypeVar, Union
-)
+from typing import (Any, Callable, Dict, Hashable, Iterable, List, Optional,
+                    Sequence, Set, Tuple, TypeVar, Union)
 
 from merkle_json import MerkleJson
 from munch import Munch as Bunch
 import pandas as pd
 import pydantic
 
+from trulens_eval.keys import redact_value
+
 logger = logging.getLogger(__name__)
 pp = PrettyPrinter()
 
 T = TypeVar("T")
 
-UNICODE_STOP = "🛑"
-UNICODE_CHECK = "✅"
-UNICODE_YIELD = "⚡"
-UNICODE_HOURGLASS = "⏳"
-UNICODE_CLOCK = "⏰"
 
 # Optional requirements.
 
 langchain_version = "0.0.230"
 
 REQUIREMENT_LLAMA = (
     "llama_index 0.6.24 or above is required for instrumenting llama_index apps. "
-    "Please install it before use: `pip install llama_index>=0.6.24`."
+    "Please install it before use: `pip install llama_index>=0.7.0`."
 )
 REQUIREMENT_LANGCHAIN = (
     f"langchain {langchain_version} or above is required for instrumenting langchain apps. "
     f"Please install it before use: `pip install langchain>={langchain_version}`."
 )
 
 
@@ -272,28 +271,44 @@
 
 # Key of structure where class information is stored. See WithClassInfo mixin.
 CLASS_INFO = "__tru_class_info"
 
 ALL_SPECIAL_KEYS = set([CIRCLE, ERROR, CLASS_INFO, NOSERIO])
 
 
-def _safe_getattr(obj, k):
+def _safe_getattr(obj: Any, k: str) -> Any:
+    """
+    Try to get the attribute `k` of the given object. This may evaluate some
+    code if the attribute is a property and may fail. In that case, an dict
+    indicating so is returned.
+    """
+
     v = inspect.getattr_static(obj, k)
 
     if isinstance(v, property):
         try:
             v = v.fget(obj)
             return v
         except Exception as e:
             return {ERROR: ObjSerial.of_object(e)}
     else:
         return v
 
 
-def _clean_attributes(obj):
+def _clean_attributes(obj) -> Dict[str, Any]:
+    """
+    Determine which attributes of the given object should be enumerated for
+    storage and/or display in UI. Returns a dict of those attributes and their
+    values.
+
+    For enumerating contents of objects that do not support utility classes like
+    pydantic, we use this method to guess what should be enumerated when
+    serializing/displaying.
+    """
+
     keys = dir(obj)
 
     ret = {}
 
     for k in keys:
         if k.startswith("__"):
             # These are typically very internal components not meant to be
@@ -313,15 +328,16 @@
 
 
 # TODO: refactor to somewhere else or change instrument to a generic filter
 def jsonify(
     obj: Any,
     dicted: Optional[Dict[int, JSON]] = None,
     instrument: Optional['Instrument'] = None,
-    skip_specials: bool = False
+    skip_specials: bool = False,
+    redact_keys: bool = False
 ) -> JSON:
     """
     Convert the given object into types that can be serialized in json.
 
     Args:
 
         - obj: Any -- the object to jsonify.
@@ -332,14 +348,17 @@
         - instrument: Optional[Instrument] -- instrumentation functions for
           checking whether to recur into components of `obj`.
 
         - skip_specials: bool (default is False) -- if set, will remove
           specially keyed structures from the json. These have keys that start
           with "__tru_".
 
+        - redact_keys: bool (default is False) -- if set, will redact secrets
+          from the output. Secrets are detremined by `keys.py:redact_value` .
+
     Returns:
 
         JSON | Sequence[JSON]
     """
 
     from trulens_eval.instruments import Instrument
 
@@ -354,39 +373,49 @@
     if id(obj) in dicted:
         if skip_specials:
             return None
         else:
             return {CIRCLE: id(obj)}
 
     if isinstance(obj, JSON_BASES):
-        return obj
+        if redact_keys and isinstance(obj, str):
+            return redact_value(obj)
+        else:
+            return obj
 
     if isinstance(obj, Path):
         return str(obj)
 
     if type(obj) in pydantic.json.ENCODERS_BY_TYPE:
         return obj
 
     # TODO: should we include duplicates? If so, dicted needs to be adjusted.
     new_dicted = {k: v for k, v in dicted.items()}
 
     recur = lambda o: jsonify(
         obj=o,
         dicted=new_dicted,
         instrument=instrument,
-        skip_specials=skip_specials
+        skip_specials=skip_specials,
+        redact_keys=redact_keys
     )
 
     if isinstance(obj, Enum):
         return obj.name
 
     if isinstance(obj, Dict):
         temp = {}
         new_dicted[id(obj)] = temp
         temp.update({k: recur(v) for k, v in obj.items() if recur_key(k)})
+
+        # Redact possible secrets based on key name and value.
+        if redact_keys:
+            for k, v in temp.items():
+                temp[k] = redact_value(v=v, k=k)
+
         return temp
 
     elif isinstance(obj, Sequence):
         temp = []
         new_dicted[id(obj)] = temp
         for x in (recur(v) for v in obj):
             temp.append(x)
@@ -397,31 +426,42 @@
         new_dicted[id(obj)] = temp
         for x in (recur(v) for v in obj):
             temp.append(x)
         return temp
 
     elif isinstance(obj, pydantic.BaseModel):
         # Not even trying to use pydantic.dict here.
+
         temp = {}
         new_dicted[id(obj)] = temp
         temp.update(
             {
                 k: recur(_safe_getattr(obj, k))
                 for k, v in obj.__fields__.items()
                 if not v.field_info.exclude and recur_key(k)
             }
         )
+
+        # Redact possible secrets based on key name and value.
+        if redact_keys:
+            for k, v in temp.items():
+                temp[k] = redact_value(v=v, k=k)
+
         if instrument.to_instrument_object(obj):
             temp[CLASS_INFO] = Class.of_class(
                 cls=obj.__class__, with_bases=True
             ).dict()
 
         return temp
 
     elif obj.__class__.__module__.startswith("llama_index."):
+        # Most of llama_index classes do not inherit a storage-utility class
+        # like pydantc so we have to enumerate their contents ourselves based on
+        # some heuristics.
+
         temp = {}
         new_dicted[id(obj)] = temp
 
         kvs = _clean_attributes(obj)
 
         temp.update(
             {
@@ -1139,17 +1179,14 @@
 
         return pd.DataFrame(rows, columns=["alive", "thread"])
 
 
 # python instrumentation utilities
 
 
-def caller_frame(offset=0):
-    return stack()[offset + 1].frame
-
 
 def get_local_in_call_stack(
     key: str,
     func: Callable[[Callable], bool],
     offset: int = 1
 ) -> Optional[Any]:
     """
@@ -1199,17 +1236,25 @@
 
 
 class Module(SerialModel):
     package_name: Optional[str]  # some modules are not in a package
     module_name: str
 
     def of_module(mod: ModuleType, loadable: bool = False) -> 'Module':
+        if loadable and mod.__name__ == "__main__":
+            # running in notebook
+            raise ImportError(f"Module {mod} is not importable.")
+
         return Module(package_name=mod.__package__, module_name=mod.__name__)
 
     def of_module_name(module_name: str, loadable: bool = False) -> 'Module':
+        if loadable and module_name == "__main__":
+            # running in notebook
+            raise ImportError(f"Module {module_name} is not importable.")
+
         mod = importlib.import_module(module_name)
         package_name = mod.__package__
         return Module(package_name=package_name, module_name=module_name)
 
     def load(self) -> ModuleType:
         return importlib.import_module(
             self.module_name, package=self.package_name
@@ -1242,24 +1287,38 @@
         module_name = self.module.module_name
         for base in self.bases[::-1]:
             if base.module.module_name == module_name:
                 return base
 
         return self
 
+    def _check_importable(self):
+        try:
+            cls = self.load()
+        except Exception as e:
+            raise ImportError(
+                f"Class {self} is not importable. "
+                "If you are defining custom feedback function implementations, make sure they can be imported by python scripts. "
+                "If you defined a function in a notebook, it will not be importable."
+            )
+
     @staticmethod
     def of_class(
         cls: type, with_bases: bool = False, loadable: bool = False
     ) -> 'Class':
-        return Class(
+        ret = Class(
             name=cls.__name__,
-            module=Module.of_module_name(cls.__module__),
+            module=Module.of_module_name(cls.__module__, loadable=loadable),
             bases=list(map(lambda base: Class.of_class(cls=base), cls.__mro__))
             if with_bases else None
         )
+        if loadable:
+            ret._check_importable()
+
+        return ret
 
     @staticmethod
     def of_object(
         obj: object, with_bases: bool = False, loadable: bool = False
     ):
         return Class.of_class(
             cls=obj.__class__, with_bases=with_bases, loadable=loadable
@@ -1335,24 +1394,27 @@
             return Obj(**d)
 
     @staticmethod
     def of_object(
         obj: object,
         cls: Optional[type] = None,
         loadable: bool = False
-    ) -> Union['Obj', 'ObjSerial']:
+    ) -> Union['Obj', 'ObjSerial']:        
         if loadable:
             return ObjSerial.of_object(obj=obj, cls=cls, loadable=loadable)
-
+        
         if cls is None:
             cls = obj.__class__
-
+        
         return Obj(cls=Class.of_class(cls), id=id(obj))
 
     def load(self) -> object:
+        # Check that the object's class is importable before the other error is thrown.
+        self.cls._check_importable()
+
         raise RuntimeError(
             f"Trying to load an object without constructor arguments: {pp.pformat(self.dict())}."
         )
 
 
 class Bindings(SerialModel):
     args: Tuple
@@ -1397,26 +1459,42 @@
         if isinstance(obj, pydantic.BaseModel):
             init_args = ()
             init_kwargs = obj.dict()
         elif isinstance(obj, Exception):
             init_args = obj.args
             init_kwargs = {}
         else:
+            # For unknown types, check if the constructor for cls expect
+            # arguments and fail if so as we don't know where to get them. If
+            # there are none, create empty init bindings.
+
+            sig = _safe_init_sig(cls)
+            if len(sig.parameters) > 0:
+                raise RuntimeError(
+                    f"Do not know how to get constructor arguments for object of type {cls.__name__}. "
+                    f"If you are defining a custom feedback function, define its implementation as a function or a method of a Provider subclass."
+                )
+
             init_args = ()
             init_kwargs = {}
+
         # TODO: dataclasses
         # TODO: dataclasses_json
 
-        sig = _safe_init_sig(cls)
+        sig = _safe_init_sig(cls.__call__)
+        # NOTE: Something related to pydantic models incorrectly sets signature
+        # of cls so we need to check cls.__call__ instead.
+        
         b = sig.bind(*init_args, **init_kwargs)
         bindings = Bindings.of_bound_arguments(b)
 
-        return ObjSerial(
-            cls=Class.of_class(cls), id=id(obj), init_bindings=bindings
-        )
+        cls_serial = Class.of_class(cls)
+        cls_serial._check_importable()
+
+        return ObjSerial(cls=cls_serial, id=id(obj), init_bindings=bindings)
 
     def load(self) -> object:
         cls = self.cls.load()
 
         sig = _safe_init_sig(cls)
         bindings = self.init_bindings.load(sig)
 
@@ -1445,19 +1523,25 @@
         if isinstance(d, Dict):
             return FunctionOrMethod.pick(**d)
         else:
             return d
 
     @staticmethod
     def of_callable(c: Callable, loadable: bool = False) -> 'FunctionOrMethod':
-        if hasattr(c, "__self__"):
-            return Method.of_method(
-                c, obj=getattr(c, "__self__"), loadable=loadable
-            )
+        """
+        Serialize the given callable. If `loadable` is set, tries to add enough
+        info for the callable to be deserialized.
+        """
+
+        if inspect.ismethod(c):
+            self = c.__self__
+            return Method.of_method(c, obj=self, loadable=loadable)
+        
         else:
+            
             return Function.of_function(c, loadable=loadable)
 
     def load(self) -> Callable:
         raise NotImplementedError()
 
 
 class Method(FunctionOrMethod):
@@ -1474,38 +1558,48 @@
     def of_method(
         meth: Callable,
         cls: Optional[type] = None,
         obj: Optional[object] = None,
         loadable: bool = False
     ) -> 'Method':
         if obj is None:
-            assert hasattr(
-                meth, "__self__"
+            assert inspect.ismethod(
+                meth
             ), f"Expected a method (maybe it is a function?): {meth}"
             obj = meth.__self__
 
         if cls is None:
-            cls = obj.__class__
+            if isinstance(cls, type):
+                # classmethod, self is a type
+                cls = obj
+            else:
+                # normal method, self is instance of cls
+                cls = obj.__class__
 
         obj_json = (ObjSerial if loadable else Obj).of_object(obj, cls=cls)
 
         return Method(obj=obj_json, name=meth.__name__)
 
     def load(self) -> Callable:
         obj = self.obj.load()
         return getattr(obj, self.name)
 
 
 class Function(FunctionOrMethod):
     """
-    A python function.
+    A python function. Could be a static method inside a class (not instance of
+    the class).
     """
 
     module: Module
+
+    # For static methods in a class which we view as functions, not yet
+    # supported:
     cls: Optional[Class]
+
     name: str
 
     @staticmethod
     def of_function(
         func: Callable,
         module: Optional[ModuleType] = None,
         cls: Optional[type] = None,
@@ -1518,19 +1612,29 @@
         if cls is not None:
             cls = Class.of_class(cls, loadable=loadable)
 
         return Function(cls=cls, module=module, name=func.__name__)
 
     def load(self) -> Callable:
         if self.cls is not None:
-            cls = self.cls.load()
-            return getattr(cls, self.name)
+            # TODO: static/class methods work in progress
+
+            cls = self.cls.load()  # does not create object instance
+            return getattr(cls, self.name)  # lookup static/class method
+        
         else:
             mod = self.module.load()
-            return getattr(mod, self.name)
+            try:
+                return getattr(mod, self.name)  # function not inside a class
+            except Exception:
+                raise ImportError(
+                   f"Function {self} is not importable. "
+                    "If you are defining custom feedback function implementations, make sure they can be imported by python scripts. "
+                    "If you defined a function in a notebook, it will not be importable."
+                )
 
 
 class WithClassInfo(pydantic.BaseModel):
     """
     Mixin to track class information to aid in querying serialized components
     without having to load them.
     """
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## trulens_eval/pages/Evaluations.py

```diff
@@ -6,15 +6,15 @@
 from st_aggrid import AgGrid
 from st_aggrid.grid_options_builder import GridOptionsBuilder
 from st_aggrid.shared import GridUpdateMode
 from st_aggrid.shared import JsCode
 import streamlit as st
 from streamlit_javascript import st_javascript
 from ux.add_logo import add_logo
-from ux.styles import default_pass_fail_color_threshold
+from ux.styles import CATEGORY
 
 from trulens_eval import Tru
 from trulens_eval.app import ComponentView
 from trulens_eval.app import instrumented_component_views
 from trulens_eval.app import LLM
 from trulens_eval.app import Other
 from trulens_eval.app import Prompt
@@ -22,15 +22,14 @@
 from trulens_eval.schema import Record
 from trulens_eval.schema import Select
 from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 from trulens_eval.ux.components import draw_call
 from trulens_eval.ux.components import draw_llm_info
 from trulens_eval.ux.components import draw_prompt_info
-from trulens_eval.ux.components import draw_selector_button
 from trulens_eval.ux.components import render_selector_markdown
 from trulens_eval.ux.components import write_or_json
 from trulens_eval.ux.styles import cellstyle_jscode
 
 st.set_page_config(page_title="Evaluations", layout="wide")
 
 st.title("Evaluations")
@@ -60,14 +59,18 @@
             console.error("Async: Could not copy text: ", err)
         }}
     )
 """
     )
 
 
+def jsonify_for_ui(*args, **kwargs):
+    return jsonify(*args, **kwargs, redact_keys=True, skip_specials=True)
+
+
 def render_component(query, component, header=True):
     # Draw the accessor/path within the wrapped app of the component.
     if header:
         st.subheader(
             f"Component {render_selector_markdown(Select.for_app(query))}"
         )
 
@@ -84,19 +87,19 @@
         draw_llm_info(component=component, query=query)
 
     elif isinstance(component, Prompt):
         draw_prompt_info(component=component, query=query)
 
     elif isinstance(component, Other):
         with st.expander("Uncategorized Component Details:"):
-            st.json(jsonify(component.json, skip_specials=True))
+            st.json(jsonify_for_ui(component.json))
 
     else:
         with st.expander("Unhandled Component Details:"):
-            st.json(jsonify(component.json, skip_specials=True))
+            st.json(jsonify_for_ui(component.json))
 
 
 if df_results.empty:
     st.write("No records yet...")
 
 else:
     apps = list(df_results.app_id.unique())
@@ -205,31 +208,35 @@
                 feedback_name = fcol
                 feedback_result = row[fcol]
                 feedback_calls = row[f"{fcol}_calls"]
 
                 def display_feedback_call(call):
 
                     def highlight(s):
-                        return ['background-color: #4CAF50'] * len(
-                            s
-                        ) if s.result >= default_pass_fail_color_threshold else [
-                            'background-color: #FCE6E6'
-                        ] * len(s)
-
+                        cat = CATEGORY.of_score(s.result)
+                        return [f'background-color: {cat.color}'] * len(s)
+                        
                     if call is not None and len(call) > 0:
+                        
                         df = pd.DataFrame.from_records(
                             [call[i]["args"] for i in range(len(call))]
                         )
                         df["result"] = pd.DataFrame(
                             [float(call[i]["ret"]) for i in range(len(call))]
                         )
+                        df["meta"] = pd.Series(
+                            [call[i]["meta"] for i in range(len(call))]
+                        )
+                        df = df.join(df.meta.apply(lambda m: pd.Series(m))).drop(columns="meta")
+                        
                         st.dataframe(
                             df.style.apply(highlight, axis=1
                                           ).format("{:.2}", subset=["result"])
                         )
+
                     else:
                         st.text("No feedback details.")
 
                 with st.expander(f"{feedback_name} = {feedback_result}",
                                  expanded=True):
                     display_feedback_call(feedback_calls)
 
@@ -265,15 +272,15 @@
 
                     st.subheader(
                         f"{app_call.method.obj.cls.name} {render_selector_markdown(Select.for_app(match_query))}"
                     )
 
                     draw_call(match)
                     # with st.expander("Call Details:"):
-                    #     st.json(jsonify(match, skip_specials=True))
+                    #     st.json(jsonify_for_ui(match)
 
                     view = classes_map.get(match_query)
                     if view is not None:
                         render_component(
                             query=match_query, component=view, header=False
                         )
                     else:
@@ -288,15 +295,15 @@
                                 st.json(app_component_json)
 
                 else:
                     st.text('No match found')
             else:
                 st.subheader(f"App {render_selector_markdown(Select.App)}")
                 with st.expander("App Details:"):
-                    st.json(jsonify(app_json, skip_specials=True))
+                    st.json(jsonify_for_ui(app_json))
 
             if match_query is not None:
                 st.header("Subcomponents:")
 
                 for query, component in classes:
                     if not match_query.is_immediate_prefix_of(query):
                         continue
@@ -307,19 +314,19 @@
 
                     render_component(query, component)
 
             st.header("More options:")
 
             if st.button("Display full app json"):
 
-                st.write(jsonify(app_json, skip_specials=True))
+                st.write(jsonify_for_ui(app_json))
 
             if st.button("Display full record json"):
 
-                st.write(jsonify(record_json, skip_specials=True))
+                st.write(jsonify_for_ui(record_json))
 
     with tab2:
         feedback = feedback_cols
         cols = 4
         rows = len(feedback) // cols + 1
 
         for row_num in range(rows):
```

## trulens_eval/pages/Progress.py

```diff
@@ -10,23 +10,29 @@
 import streamlit as st
 from ux.add_logo import add_logo
 
 from trulens_eval import db
 from trulens_eval import Tru
 from trulens_eval.db import DB
 from trulens_eval.feedback import Feedback
-from trulens_eval.keys import *
 from trulens_eval.provider_apis import Endpoint
 from trulens_eval.provider_apis import HuggingfaceEndpoint
 from trulens_eval.provider_apis import OpenAIEndpoint
 from trulens_eval.schema import FeedbackResultStatus
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
 from trulens_eval.util import TP
 
+from trulens_eval.keys import check_keys
+
+check_keys(
+    "OPENAI_API_KEY",
+    "HUGGINGFACE_API_KEY"
+)
+
 st.set_page_config(page_title="Feedback Progress", layout="wide")
 
 st.title("Feedback Progress")
 
 st.runtime.legacy_caching.clear_cache()
 
 add_logo()
```

## trulens_eval/utils/llama.py

```diff
@@ -9,15 +9,15 @@
 from trulens_eval.util import JSON
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LLAMA
 from trulens_eval.util import second
 from trulens_eval.util import TP
 
 with OptionalImports(message=REQUIREMENT_LLAMA):
-    from llama_index.data_structs.node import NodeWithScore
+    from llama_index.schema import NodeWithScore
     from llama_index.indices.query.schema import QueryBundle
     from llama_index.indices.vector_store.retrievers import \
         VectorIndexRetriever
 
 
 class Prompt(app.Prompt, app.LangChainComponent):
```

## trulens_eval/ux/components.py

```diff
@@ -6,14 +6,15 @@
 import streamlit as st
 from streamlit_javascript import st_javascript
 
 from trulens_eval.app import ComponentView
 from trulens_eval.schema import Record
 from trulens_eval.schema import RecordAppCall
 from trulens_eval.schema import Select
+from trulens_eval.keys import REDACTED_VALUE, should_redact_key
 from trulens_eval.util import CLASS_INFO
 from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import is_empty
 from trulens_eval.util import is_noserio
 from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 
@@ -153,14 +154,22 @@
                 <style>
                 thead tr th:first-child {display:none}
                 tbody th {display:none}
                 </style>
                 """
     df = pd.DataFrame.from_dict(llm_kv, orient='index').transpose()
 
+    # Redact any column whose name indicates it might be a secret.
+    for col in df.columns:
+        if should_redact_key(col):
+            df[col] = REDACTED_VALUE
+
+    # TODO: What about columns not indicating a secret but some values do
+    # indicate it as per `should_redact_value` ?
+
     # Iterate over each column of the DataFrame
     for column in df.columns:
         path = getattr(Select.for_app(query), str(column))
         # Check if any cell in the column is a dictionary
 
         if any(isinstance(cell, dict) for cell in df[column]):
             # Create new columns for each key in the dictionary
```

## trulens_eval/ux/styles.py

```diff
@@ -1,76 +1,145 @@
-00000000: 6672 6f6d 2074 7275 6c65 6e73 5f65 7661  from trulens_eva
-00000010: 6c2e 6665 6564 6261 636b 2069 6d70 6f72  l.feedback impor
-00000020: 7420 6465 6661 756c 745f 7061 7373 5f66  t default_pass_f
-00000030: 6169 6c5f 636f 6c6f 725f 7468 7265 7368  ail_color_thresh
-00000040: 6f6c 640a 0a23 2054 6865 7365 2077 6f75  old..# These wou
-00000050: 6c64 2062 6520 7573 6566 756c 2074 6f20  ld be useful to 
-00000060: 696e 636c 7564 6520 696e 206f 7572 2070  include in our p
-00000070: 6167 6573 2062 7574 2064 6f6e 2774 2079  ages but don't y
-00000080: 6574 2073 6565 2061 2077 6179 2074 6f20  et see a way to 
-00000090: 646f 2074 6869 7320 696e 2073 7472 6561  do this in strea
-000000a0: 6d6c 6974 2e0a 726f 6f74 5f6a 7320 3d20  mlit..root_js = 
-000000b0: 6622 2222 0a20 2020 2064 6566 6175 6c74  f""".    default
-000000c0: 5f70 6173 735f 6661 696c 5f63 6f6c 6f72  _pass_fail_color
-000000d0: 5f74 6872 6573 686f 6c64 203d 207b 6465  _threshold = {de
-000000e0: 6661 756c 745f 7061 7373 5f66 6169 6c5f  fault_pass_fail_
-000000f0: 636f 6c6f 725f 7468 7265 7368 6f6c 647d  color_threshold}
-00000100: 3b0a 2222 220a 0a72 6f6f 745f 6874 6d6c  ;."""..root_html
-00000110: 203d 2066 2222 220a 6a73 3a0a 3c73 6372   = f""".js:.<scr
-00000120: 6970 743e 0a20 2020 207b 726f 6f74 5f6a  ipt>.    {root_j
-00000130: 737d 0a3c 2f73 6372 6970 743e 0a22 2222  s}.</script>."""
-00000140: 0a0a 7374 6d65 7472 6963 6465 6c74 615f  ..stmetricdelta_
-00000150: 6869 6465 6172 726f 7720 3d20 2222 220a  hidearrow = """.
-00000160: 2020 2020 3c73 7479 6c65 3e20 5b64 6174      <style> [dat
-00000170: 612d 7465 7374 6964 3d22 7374 4d65 7472  a-testid="stMetr
-00000180: 6963 4465 6c74 6122 5d20 7376 6720 7b20  icDelta"] svg { 
-00000190: 6469 7370 6c61 793a 206e 6f6e 653b 207d  display: none; }
-000001a0: 203c 2f73 7479 6c65 3e0a 2020 2020 2222   </style>.    ""
-000001b0: 220a 0a63 656c 6c73 7479 6c65 5f6a 7363  "..cellstyle_jsc
-000001c0: 6f64 6520 3d20 2222 220a 2020 2020 6675  ode = """.    fu
-000001d0: 6e63 7469 6f6e 2870 6172 616d 7329 207b  nction(params) {
-000001e0: 0a20 2020 2020 2020 2069 6620 2870 6172  .        if (par
-000001f0: 7365 466c 6f61 7428 7061 7261 6d73 2e76  seFloat(params.v
-00000200: 616c 7565 2920 3c20 2222 2220 2b20 7374  alue) < """ + st
-00000210: 7228 0a20 2020 2064 6566 6175 6c74 5f70  r(.    default_p
-00000220: 6173 735f 6661 696c 5f63 6f6c 6f72 5f74  ass_fail_color_t
-00000230: 6872 6573 686f 6c64 0a29 202b 2022 2222  hreshold.) + """
-00000240: 2920 7b0a 2020 2020 2020 2020 2020 2020  ) {.            
-00000250: 7265 7475 726e 207b 0a20 2020 2020 2020  return {.       
-00000260: 2020 2020 2020 2020 2027 636f 6c6f 7227           'color'
-00000270: 3a20 2762 6c61 636b 272c 0a20 2020 2020  : 'black',.     
-00000280: 2020 2020 2020 2020 2020 2027 6261 636b             'back
-00000290: 6772 6f75 6e64 436f 6c6f 7227 3a20 2723  groundColor': '#
-000002a0: 4643 4536 4536 270a 2020 2020 2020 2020  FCE6E6'.        
-000002b0: 2020 2020 7d0a 2020 2020 2020 2020 7d20      }.        } 
-000002c0: 656c 7365 2069 6620 2870 6172 7365 466c  else if (parseFl
-000002d0: 6f61 7428 7061 7261 6d73 2e76 616c 7565  oat(params.value
-000002e0: 2920 3e3d 2022 2222 202b 2073 7472 280a  ) >= """ + str(.
-000002f0: 2020 2020 6465 6661 756c 745f 7061 7373      default_pass
-00000300: 5f66 6169 6c5f 636f 6c6f 725f 7468 7265  _fail_color_thre
-00000310: 7368 6f6c 640a 2920 2b20 2222 2229 207b  shold.) + """) {
-00000320: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret
-00000330: 7572 6e20 7b0a 2020 2020 2020 2020 2020  urn {.          
-00000340: 2020 2020 2020 2763 6f6c 6f72 273a 2027        'color': '
-00000350: 626c 6163 6b27 2c0a 2020 2020 2020 2020  black',.        
-00000360: 2020 2020 2020 2020 2762 6163 6b67 726f          'backgro
-00000370: 756e 6443 6f6c 6f72 273a 2027 2334 4341  undColor': '#4CA
-00000380: 4635 3027 0a20 2020 2020 2020 2020 2020  F50'.           
-00000390: 207d 0a20 2020 2020 2020 207d 2065 6c73   }.        } els
-000003a0: 6520 7b0a 2020 2020 2020 2020 2020 2020  e {.            
-000003b0: 7265 7475 726e 207b 0a20 2020 2020 2020  return {.       
-000003c0: 2020 2020 2020 2020 2027 636f 6c6f 7227           'color'
-000003d0: 3a20 2762 6c61 636b 272c 0a20 2020 2020  : 'black',.     
-000003e0: 2020 2020 2020 2020 2020 2027 6261 636b             'back
-000003f0: 6772 6f75 6e64 436f 6c6f 7227 3a20 2777  groundColor': 'w
-00000400: 6869 7465 270a 2020 2020 2020 2020 2020  hite'.          
-00000410: 2020 7d0a 2020 2020 2020 2020 7d0a 2020    }.        }.  
-00000420: 2020 7d3b 0a20 2020 2022 2222 0a0a 6869    };.    """..hi
-00000430: 6465 5f74 6162 6c65 5f72 6f77 5f69 6e64  de_table_row_ind
-00000440: 6578 203d 2022 2222 0a20 2020 203c 7374  ex = """.    <st
-00000450: 796c 653e 0a20 2020 2020 2020 2074 6865  yle>.        the
-00000460: 6164 2074 7220 7468 3a66 6972 7374 2d63  ad tr th:first-c
-00000470: 6869 6c64 207b 6469 7370 6c61 793a 6e6f  hild {display:no
-00000480: 6e65 7d0a 2020 2020 2020 2020 7462 6f64  ne}.        tbod
-00000490: 7920 7468 207b 6469 7370 6c61 793a 6e6f  y th {display:no
-000004a0: 6e65 7d0a 2020 2020 3c2f 7374 796c 653e  ne}.    </style>
-000004b0: 0a20 2020 2022 2222 0a                   .    """.
+00000000: 696d 706f 7274 206e 756d 7079 2061 7320  import numpy as 
+00000010: 6e70 0a0a 6672 6f6d 2074 7275 6c65 6e73  np..from trulens
+00000020: 5f65 7661 6c2e 7574 696c 2069 6d70 6f72  _eval.util impor
+00000030: 7420 5365 7269 616c 4d6f 6465 6c0a 0a0a  t SerialModel...
+00000040: 636c 6173 7320 4341 5445 474f 5259 3a0a  class CATEGORY:.
+00000050: 2020 2020 2222 220a 2020 2020 4665 6564      """.    Feed
+00000060: 6261 636b 2072 6573 756c 7420 6361 7465  back result cate
+00000070: 676f 7269 6573 2066 6f72 2064 6973 706c  gories for displ
+00000080: 6179 696e 6720 7075 7270 6f73 6573 3a20  aying purposes: 
+00000090: 7061 7373 2c20 7761 726e 696e 672c 2066  pass, warning, f
+000000a0: 6169 6c2c 206f 720a 2020 2020 756e 6b6e  ail, or.    unkn
+000000b0: 6f77 6e2e 0a20 2020 2022 2222 0a0a 2020  own..    """..  
+000000c0: 2020 636c 6173 7320 4361 7465 676f 7279    class Category
+000000d0: 2853 6572 6961 6c4d 6f64 656c 293a 0a20  (SerialModel):. 
+000000e0: 2020 2020 2020 206e 616d 653a 2073 7472         name: str
+000000f0: 0a20 2020 2020 2020 2061 646a 6563 7469  .        adjecti
+00000100: 7665 3a20 7374 720a 2020 2020 2020 2020  ve: str.        
+00000110: 7468 7265 7368 6f6c 643a 2066 6c6f 6174  threshold: float
+00000120: 0a20 2020 2020 2020 2063 6f6c 6f72 3a20  .        color: 
+00000130: 7374 720a 2020 2020 2020 2020 6963 6f6e  str.        icon
+00000140: 3a20 7374 720a 0a20 2020 2050 4153 5320  : str..    PASS 
+00000150: 3d20 4361 7465 676f 7279 280a 2020 2020  = Category(.    
+00000160: 2020 2020 6e61 6d65 3d22 7061 7373 222c      name="pass",
+00000170: 2061 646a 6563 7469 7665 3d22 6869 6768   adjective="high
+00000180: 222c 2074 6872 6573 686f 6c64 3d30 2e38  ", threshold=0.8
+00000190: 2c20 636f 6c6f 723d 2223 6161 6666 6161  , color="#aaffaa
+000001a0: 222c 2069 636f 6e3d 22e2 9c85 220a 2020  ", icon="...".  
+000001b0: 2020 290a 2020 2020 5741 524e 494e 4720    ).    WARNING 
+000001c0: 3d20 4361 7465 676f 7279 280a 2020 2020  = Category(.    
+000001d0: 2020 2020 6e61 6d65 3d22 7761 726e 696e      name="warnin
+000001e0: 6722 2c0a 2020 2020 2020 2020 6164 6a65  g",.        adje
+000001f0: 6374 6976 653d 226d 6564 6975 6d22 2c0a  ctive="medium",.
+00000200: 2020 2020 2020 2020 7468 7265 7368 6f6c          threshol
+00000210: 643d 302e 362c 0a20 2020 2020 2020 2063  d=0.6,.        c
+00000220: 6f6c 6f72 3d22 2366 6666 6661 6122 2c0a  olor="#ffffaa",.
+00000230: 2020 2020 2020 2020 6963 6f6e 3d22 e29a          icon="..
+00000240: a0ef b88f 220a 2020 2020 290a 2020 2020  ....".    ).    
+00000250: 4641 494c 203d 2043 6174 6567 6f72 7928  FAIL = Category(
+00000260: 0a20 2020 2020 2020 206e 616d 653d 2266  .        name="f
+00000270: 6169 6c22 2c20 6164 6a65 6374 6976 653d  ail", adjective=
+00000280: 226c 6f77 222c 2074 6872 6573 686f 6c64  "low", threshold
+00000290: 3d30 2e30 2c20 636f 6c6f 723d 2223 6666  =0.0, color="#ff
+000002a0: 6161 6161 222c 2069 636f 6e3d 22f0 9f9b  aaaa", icon="...
+000002b0: 9122 0a20 2020 2029 0a20 2020 2055 4e4b  .".    ).    UNK
+000002c0: 4e4f 574e 203d 2043 6174 6567 6f72 7928  NOWN = Category(
+000002d0: 0a20 2020 2020 2020 206e 616d 653d 2275  .        name="u
+000002e0: 6e6b 6e6f 776e 222c 0a20 2020 2020 2020  nknown",.       
+000002f0: 2061 646a 6563 7469 7665 3d22 756e 6b6e   adjective="unkn
+00000300: 6f77 6e22 2c0a 2020 2020 2020 2020 7468  own",.        th
+00000310: 7265 7368 6f6c 643d 6e70 2e6e 616e 2c0a  reshold=np.nan,.
+00000320: 2020 2020 2020 2020 636f 6c6f 723d 2223          color="#
+00000330: 6161 6161 6161 222c 0a20 2020 2020 2020  aaaaaa",.       
+00000340: 2069 636f 6e3d 223f 220a 2020 2020 290a   icon="?".    ).
+00000350: 0a20 2020 2041 4c4c 203d 205b 5041 5353  .    ALL = [PASS
+00000360: 2c20 5741 524e 494e 472c 2046 4149 4c5d  , WARNING, FAIL]
+00000370: 2023 206e 6f74 2069 6e63 6c75 6469 6e67   # not including
+00000380: 2055 4e4b 4e4f 574e 2069 6e74 656e 7469   UNKNOWN intenti
+00000390: 6f6e 616c 6c79 0a0a 2020 2020 4073 7461  onally..    @sta
+000003a0: 7469 636d 6574 686f 640a 2020 2020 6465  ticmethod.    de
+000003b0: 6620 6f66 5f73 636f 7265 2873 636f 7265  f of_score(score
+000003c0: 3a20 666c 6f61 7429 202d 3e20 4361 7465  : float) -> Cate
+000003d0: 676f 7279 3a0a 2020 2020 2020 2020 666f  gory:.        fo
+000003e0: 7220 6361 7420 696e 2043 4154 4547 4f52  r cat in CATEGOR
+000003f0: 592e 414c 4c3a 0a20 2020 2020 2020 2020  Y.ALL:.         
+00000400: 2020 2069 6620 7363 6f72 6520 3e3d 2063     if score >= c
+00000410: 6174 2e74 6872 6573 686f 6c64 3a0a 2020  at.threshold:.  
+00000420: 2020 2020 2020 2020 2020 2020 2020 7265                re
+00000430: 7475 726e 2063 6174 0a0a 2020 2020 2020  turn cat..      
+00000440: 2020 7265 7475 726e 2043 4154 4547 4f52    return CATEGOR
+00000450: 592e 554e 4b4e 4f57 4e0a 0a0a 2320 5468  Y.UNKNOWN...# Th
+00000460: 6573 6520 776f 756c 6420 6265 2075 7365  ese would be use
+00000470: 6675 6c20 746f 2069 6e63 6c75 6465 2069  ful to include i
+00000480: 6e20 6f75 7220 7061 6765 7320 6275 7420  n our pages but 
+00000490: 646f 6e27 7420 7965 7420 7365 6520 6120  don't yet see a 
+000004a0: 7761 7920 746f 2064 6f0a 2320 7468 6973  way to do.# this
+000004b0: 2069 6e20 7374 7265 616d 6c69 742e 0a72   in streamlit..r
+000004c0: 6f6f 745f 6a73 203d 2066 2222 220a 2020  oot_js = f""".  
+000004d0: 2020 7661 7220 6465 6661 756c 745f 7061    var default_pa
+000004e0: 7373 5f74 6872 6573 686f 6c64 203d 207b  ss_threshold = {
+000004f0: 4341 5445 474f 5259 2e50 4153 532e 7468  CATEGORY.PASS.th
+00000500: 7265 7368 6f6c 647d 3b0a 2020 2020 7661  reshold};.    va
+00000510: 7220 6465 6661 756c 745f 7761 726e 696e  r default_warnin
+00000520: 675f 7468 7265 7368 6f6c 6420 3d20 7b43  g_threshold = {C
+00000530: 4154 4547 4f52 592e 5741 524e 494e 472e  ATEGORY.WARNING.
+00000540: 7468 7265 7368 6f6c 647d 3b0a 2020 2020  threshold};.    
+00000550: 7661 7220 6465 6661 756c 745f 6661 696c  var default_fail
+00000560: 5f74 6872 6573 686f 6c64 203d 207b 4341  _threshold = {CA
+00000570: 5445 474f 5259 2e46 4149 4c2e 7468 7265  TEGORY.FAIL.thre
+00000580: 7368 6f6c 647d 3b0a 2222 220a 0a23 204e  shold};."""..# N
+00000590: 6f74 2070 7265 7365 6e74 6c79 2075 7365  ot presently use
+000005a0: 642e 204e 6565 6420 746f 2066 6967 7572  d. Need to figur
+000005b0: 6520 6f75 7420 686f 7720 746f 2069 6e63  e out how to inc
+000005c0: 6c75 6465 2074 6869 7320 696e 2073 7472  lude this in str
+000005d0: 6561 6d6c 6974 2070 6167 6573 2e0a 726f  eamlit pages..ro
+000005e0: 6f74 5f68 746d 6c20 3d20 6622 2222 0a3c  ot_html = f""".<
+000005f0: 7363 7269 7074 3e0a 2020 2020 7b72 6f6f  script>.    {roo
+00000600: 745f 6a73 7d0a 3c2f 7363 7269 7074 3e0a  t_js}.</script>.
+00000610: 2222 220a 0a73 746d 6574 7269 6364 656c  """..stmetricdel
+00000620: 7461 5f68 6964 6561 7272 6f77 203d 2022  ta_hidearrow = "
+00000630: 2222 0a20 2020 203c 7374 796c 653e 205b  "".    <style> [
+00000640: 6461 7461 2d74 6573 7469 643d 2273 744d  data-testid="stM
+00000650: 6574 7269 6344 656c 7461 225d 2073 7667  etricDelta"] svg
+00000660: 207b 2064 6973 706c 6179 3a20 6e6f 6e65   { display: none
+00000670: 3b20 7d20 3c2f 7374 796c 653e 0a20 2020  ; } </style>.   
+00000680: 2022 2222 0a0a 6365 6c6c 7374 796c 655f   """..cellstyle_
+00000690: 6a73 636f 6465 203d 2066 2222 220a 2020  jscode = f""".  
+000006a0: 2020 6675 6e63 7469 6f6e 2870 6172 616d    function(param
+000006b0: 7329 207b 7b0a 2020 2020 2020 2020 6c65  s) {{.        le
+000006c0: 7420 7620 3d20 7061 7273 6546 6c6f 6174  t v = parseFloat
+000006d0: 2870 6172 616d 732e 7661 6c75 6529 3b0a  (params.value);.
+000006e0: 2020 2020 2020 2020 2222 2220 2b20 5c0a          """ + \.
+000006f0: 2020 2020 225c 6e22 2e6a 6f69 6e28 6622      "\n".join(f"
+00000700: 2222 0a20 2020 2020 2020 2069 6620 2876  "".        if (v
+00000710: 203e 3d20 7b63 6174 2e74 6872 6573 686f   >= {cat.thresho
+00000720: 6c64 7d29 207b 7b0a 2020 2020 2020 2020  ld}) {{.        
+00000730: 2020 2020 7265 7475 726e 207b 7b0a 2020      return {{.  
+00000740: 2020 2020 2020 2020 2020 2020 2020 2763                'c
+00000750: 6f6c 6f72 273a 2027 626c 6163 6b27 2c0a  olor': 'black',.
+00000760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000770: 2762 6163 6b67 726f 756e 6443 6f6c 6f72  'backgroundColor
+00000780: 273a 2027 7b63 6174 2e63 6f6c 6f72 7d27  ': '{cat.color}'
+00000790: 0a20 2020 2020 2020 2020 2020 207d 7d3b  .            }};
+000007a0: 0a20 2020 2020 2020 207d 7d0a 2020 2020  .        }}.    
+000007b0: 2222 2220 666f 7220 6361 7420 696e 2043  """ for cat in C
+000007c0: 4154 4547 4f52 592e 414c 4c29 202b 2066  ATEGORY.ALL) + f
+000007d0: 2222 220a 2020 2020 2020 2020 2f2f 2069  """.        // i
+000007e0: 2e65 2e20 6e6f 7420 6120 6e75 6d62 6572  .e. not a number
+000007f0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00000800: 7b7b 0a20 2020 2020 2020 2020 2020 2027  {{.            '
+00000810: 636f 6c6f 7227 3a20 2762 6c61 636b 272c  color': 'black',
+00000820: 0a20 2020 2020 2020 2020 2020 2027 6261  .            'ba
+00000830: 636b 6772 6f75 6e64 436f 6c6f 7227 3a20  ckgroundColor': 
+00000840: 277b 4341 5445 474f 5259 2e55 4e4b 4e4f  '{CATEGORY.UNKNO
+00000850: 574e 2e63 6f6c 6f72 7d27 0a20 2020 2020  WN.color}'.     
+00000860: 2020 207d 7d3b 0a20 2020 207d 7d0a 2020     }};.    }}.  
+00000870: 2020 2222 220a 0a68 6964 655f 7461 626c    """..hide_tabl
+00000880: 655f 726f 775f 696e 6465 7820 3d20 2222  e_row_index = ""
+00000890: 220a 2020 2020 3c73 7479 6c65 3e0a 2020  ".    <style>.  
+000008a0: 2020 2020 2020 7468 6561 6420 7472 2074        thead tr t
+000008b0: 683a 6669 7273 742d 6368 696c 6420 7b64  h:first-child {d
+000008c0: 6973 706c 6179 3a6e 6f6e 657d 0a20 2020  isplay:none}.   
+000008d0: 2020 2020 2074 626f 6479 2074 6820 7b64       tbody th {d
+000008e0: 6973 706c 6179 3a6e 6f6e 657d 0a20 2020  isplay:none}.   
+000008f0: 203c 2f73 7479 6c65 3e0a 2020 2020 2222   </style>.    ""
+00000900: 220a                                     ".
```

## Comparing `trulens_eval-0.5.0a0.dist-info/METADATA` & `trulens_eval-0.6.0a0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.5.0a0
+Version: 0.6.0a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
@@ -13,15 +13,15 @@
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: cohere (>=4.4.1)
 Requires-Dist: datasets (>=2.12.0)
 Requires-Dist: python-dotenv (>=1.0.0)
 Requires-Dist: kaggle (>=1.5.13)
 Requires-Dist: langchain (>=0.0.230)
-Requires-Dist: llama-index (>=0.6.24)
+Requires-Dist: llama-index (>=0.7.0)
 Requires-Dist: merkle-json (>=1.0.0)
 Requires-Dist: millify (>=0.1.1)
 Requires-Dist: openai (>=0.27.6)
 Requires-Dist: pinecone-client (>=2.2.1)
 Requires-Dist: pydantic (>=1.10.7)
 Requires-Dist: requests (>=2.30.0)
 Requires-Dist: slack-bolt (>=1.18.0)
@@ -50,32 +50,32 @@
 
 ## Quick Usage
 
 To quickly play around with the TruLens Eval library:
 
 Langchain:
 
-[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/langchain_quickstart_colab.ipynb)
+[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/colab/quickstarts/langchain_quickstart_colab.ipynb)
 
-[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/quickstart.py).
+[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/quickstart.py).
 
 Llama Index: 
 
-[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)
+[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)
 
-[llama_index_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/llama_index_quickstart.py)
+[llama_index_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/llama_index_quickstart.py)
 
 No Framework: 
 
-[no_framework_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/no_framework_quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/colab/quickstarts/no_framework_quickstart_colab.ipynb)
+[no_framework_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/no_framework_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/colab/quickstarts/no_framework_quickstart_colab.ipynb)
 
-[no_framework_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.5.0/trulens_eval/examples/no_framework_quickstart.py)
+[no_framework_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.6.0/trulens_eval/examples/no_framework_quickstart.py)
 
 ### 💡 Contributing
 
 Interested in contributing? See our [contribution guide](https://github.com/truera/trulens/tree/main/trulens_eval/CONTRIBUTING.md) for more details.
 
 ## Installation and Setup
```

## Comparing `trulens_eval-0.5.0a0.dist-info/RECORD` & `trulens_eval-0.6.0a0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,42 +1,44 @@
-trulens_eval/Example_TruBot.py,sha256=rbomfkDmYOFMJgrFLghHh9XsjhOR-TzZaGVO9_NWdZ0,5164
-trulens_eval/Leaderboard.py,sha256=_NxvxxZ1ZdVwQ5Nz-l27Qfc2IEZWEE5zYxBz5eshrNA,3265
-trulens_eval/__init__.py,sha256=eOQbVBxFVIE-LtUrYgujUoTBrFdKG84NLQHoD3kIipc,1371
-trulens_eval/app.py,sha256=EGOj-dlgOP3vMrnYd4YvUBUe4kibFZMl2OKfzw44SBQ,11727
+trulens_eval/Example_TruBot.py,sha256=RfE0j-2glu-0y5E6rxTJ_NQFmjPQUY1UeA2ygKcmPRw,5199
+trulens_eval/Leaderboard.py,sha256=64FdWzOQolUVOxSsY2ZUjybCkI7rfaOFe1ssDkXsDIc,3045
+trulens_eval/__init__.py,sha256=6Y2m2UY6ooXxyvPrLxYYqhacUqnAdxQl5ucvY1Asc_w,1424
+trulens_eval/app.py,sha256=1AxTgoOmf_EG2hCggyI-ZF8CvcJ87pobzG-tjhgVEnM,12040
 trulens_eval/benchmark.py,sha256=GI-JmBBr8KswuMaXVoJ2Rm6eAtjtwhYThcNheZakQNo,5344
-trulens_eval/db.py,sha256=W3ScGso6ya3n6CI2zTJzx6Rt4E7b_e39zJxUDw_nCdQ,21071
+trulens_eval/db.py,sha256=K-hMAqkwZcZeUdkSyt-5Yh7RuffxGKaUN-TWsK5jk4A,21224
 trulens_eval/db_migration.py,sha256=nDWmwAq2H0JmZ_I9TdK4xa2U2TA3IrwrHDCeE00cFvM,14061
-trulens_eval/feedback.py,sha256=dgemyCfFaBUMgxQn_03zRDTs_6kf2rklSQhhHCkVMCE,48952
-trulens_eval/feedback_prompts.py,sha256=DgW4_f_4g018tYNwca1D1taJhhdwaf2fDR9J8s0Upls,3443
+trulens_eval/feedback.py,sha256=7wBRXmjQ8AcWv3hrh_nSFfp3Uf6Xq4n5REzNIHtSh6s,56377
+trulens_eval/feedback_prompts.py,sha256=Un6V5d89M51xLA3K5EtVCymn6Aps0azOuYe10RfCotg,5420
 trulens_eval/instruments.py,sha256=SwHs25qeNXOU87xCSxeaPKn9JXthaO4RYc-DrlwmjWI,20145
-trulens_eval/keys.py,sha256=PzUiX6-9jKij2kfAbl-4AHYxDIPqWhpVoIMQH6zIvvs,3147
-trulens_eval/provider_apis.py,sha256=cvgbjTk1wav6zlXDp48S59poh6mbW4Sasgk_pFY6ydM,21353
-trulens_eval/schema.py,sha256=DrbjApbW__kQ-71YFYCOypNIMFKDR6S2laZSTyBiuQc,13750
-trulens_eval/tru.py,sha256=wR6t2NzbVCJTsvRvx1f3Bt6-PZC1XFAY_9PGQ8KVMKo,16226
+trulens_eval/keys.py,sha256=QxrFZT9Kb_XPtGTc8_sKAhcpQlF9xY8Szm2ObKWqj_g,13286
+trulens_eval/provider_apis.py,sha256=jAJEN4DOsxBzlGS-N8iGRnNJQVOp9HmNDwcNyThVg74,23180
+trulens_eval/schema.py,sha256=WySz8gqXHIzvv6t7qG1_3vTQgK6YJxWJgmvGnoOTftQ,13929
+trulens_eval/tru.py,sha256=APDpHuzIJqpnIqB4nMq2k5O0bDMrwg-9XqyO-CHY_mM,16238
 trulens_eval/tru_app.py,sha256=QhY0tbhif5MfkzeMzv_uwi2Dqr_POVljg-9woBq2Ep8,293
 trulens_eval/tru_basic_app.py,sha256=01sBriY4MmZ0CrogkNa6WEVaSrHj38GtpocMiCSeFCE,3545
 trulens_eval/tru_chain.py,sha256=E-96_SbJoAmpW5MDuqNBLxol0zQ1B_NOk1FOdSyBUcs,7819
 trulens_eval/tru_db.py,sha256=_S9gtV_bizaQBaEDbayBVB6ns2j11Fi-hl_Bvy6_SXg,288
 trulens_eval/tru_feedback.py,sha256=Shc6KX33QfVZjqEF2iXg6wFmXD9ANd-l6BwhMkVR8gM,318
-trulens_eval/tru_llama.py,sha256=5O3Fx_ogk0btwdxZ_Wrkkyx7mot1fKPqRZSwZJyMZAE,6370
-trulens_eval/util.py,sha256=8h6Q-IeuHFl_H0m7oId9YVP6MqARmrbU8OWJ0GZmDR0,43449
-trulens_eval/pages/Evaluations.py,sha256=cilJnesFx0iz06XBxFu8tWkJbTufM0rG4ssqIkvOprU,12673
-trulens_eval/pages/Progress.py,sha256=Nkq_-cd2U7mo7PN6_mmnQGNaSmAD26c1E9_Q77HndmY,1775
+trulens_eval/tru_llama.py,sha256=hfKe4WKmPtUCLKPH6yDcsNkNSxCF7Vo2q1DITOqoxDM,6396
+trulens_eval/util.py,sha256=X6DNlYd-80uz-jg1AzK84lxyAiZchbi-gzZ6FoZIdwg,47832
+trulens_eval/pages/Evaluations.py,sha256=Q_0HLEBciXvUfuM6H7UiFVIvoXOJCNzqVpQd1oIRC48,12802
+trulens_eval/pages/Progress.py,sha256=0U9qO4Jb0JXn1GMHm8KpIn9OtgZtoltTKsL69BLxKtw,1848
 trulens_eval/react_components/record_viewer/__init__.py,sha256=HC-OeLHjf2ULICAnIVrWP1iO4xfgZDqJKrvzIKUyV7Y,3294
 trulens_eval/react_components/record_viewer/dist/index.html,sha256=Q1dH3M-Se_PZSy3aRyN8DnbKhbH7uvq6ReclnnStxDA,411
 trulens_eval/react_components/record_viewer/dist/assets/index-13c9a784.js,sha256=03PozYu-zkT-qFuWYWLnqolOOYJS0pgJdA4sf29BlCw,470039
 trulens_eval/react_components/record_viewer/dist/assets/index-d4dfd9ae.css,sha256=1N_ZroI2walrlJwnxw5c7RzyO8z6sPS0sWT_D1lmqkM,779
-trulens_eval/tests/test_tru_chain.py,sha256=KK_yj5wVSIaRnK-LamW3g40n_g3-Ga8CC48sTB7I7yM,5551
+trulens_eval/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 trulens_eval/utils/command_line.py,sha256=K2vRIMp03IJhnevkxDnlXbn5Rux4TMXzIDgkqcePLTU,83
 trulens_eval/utils/langchain.py,sha256=y3by0IT0dYEIEqZVt3Px4deknj2YA9-jKapRd95ywMg,5892
-trulens_eval/utils/llama.py,sha256=CrqhVGNz19B29CZG2AFXoZIX9SXmxtGl_IlkGgaLll4,4842
+trulens_eval/utils/llama.py,sha256=5ygaQB9M16y_63vs_JiDtOnHFV3YYbEPZZniAXYbSfs,4831
 trulens_eval/utils/notebook_utils.py,sha256=QTB2tedjSNF5d25sfrJEg20aqcK3Kx3MfcteeWcRzxQ,1001
+trulens_eval/utils/python.py,sha256=CJq1mYK5Rxxoa6_el1igsAte00AtpHuGabGdwveQWpQ,151
+trulens_eval/utils/text.py,sha256=ExH7jnTFNsbR5gUtv98vHy-QVGoaQ4BxYtB7IWpGAmo,166
 trulens_eval/utils/trulens.py,sha256=9NQOctB0_qRONaKModZq3bPWOORMLvysAEjwt4BA1l8,927
 trulens_eval/ux/add_logo.py,sha256=lIpLNwqSGbfSP2Td6VEE6s0A1-9x4vFfXpJUxY7BFdQ,1212
-trulens_eval/ux/components.py,sha256=1NZw65TxlwR4OFNikp8Dnf8LTnZUoNqF2X_iTPMg9xM,5492
-trulens_eval/ux/styles.py,sha256=WYdJIsvUwPla1oZepc1FsmUJNPkT0vW0xt5sCSMs2qA,1209
+trulens_eval/ux/components.py,sha256=xygqpTu5hWcMEIoOKjhPGVVULs2nbjzuv9W_QZxUTsY,5847
+trulens_eval/ux/styles.py,sha256=H3sYNXEKeRDbsK-6Imu01oIXfUk158cBJMLN9FuSyXY,2306
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.5.0a0.dist-info/METADATA,sha256=1n6kr0MT9k2R2hhhiJRb2ujvAC2IaLp0E2BCMykmJV0,16347
-trulens_eval-0.5.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-trulens_eval-0.5.0a0.dist-info/entry_points.txt,sha256=EpSmkbk1fF0UH-djUia4lE2hzg1oMt1QvaVxA7SfZmo,70
-trulens_eval-0.5.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.5.0a0.dist-info/RECORD,,
+trulens_eval-0.6.0a0.dist-info/METADATA,sha256=MssrR_zIH-4fs4199YFmlmw948OpafLTU-vn3tbSfOY,16346
+trulens_eval-0.6.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.6.0a0.dist-info/entry_points.txt,sha256=EpSmkbk1fF0UH-djUia4lE2hzg1oMt1QvaVxA7SfZmo,70
+trulens_eval-0.6.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.6.0a0.dist-info/RECORD,,
```

